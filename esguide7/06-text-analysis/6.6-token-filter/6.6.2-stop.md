---
description: >-
  이 문서의 허가되지 않은 무단 복제나 배포 및 출판을 금지합니다. 본 문서의 내용 및 도표 등을 인용하고자 하는 경우 출처를 명시하고
  김종민(kimjmin@gmail.com)에게 사용 내용을 알려주시기 바랍니다.
---

## 6.6.2 Stop

&#x20; 블로그 포스트나 뉴스 기사 같은 글에는 검색에서는 큰 의미가 없는 조사나 전치사 등이 많습니다. 영문에서도 **the**, **is**, **a** 같은 단어들은 대부분 검색어로 쓰이지 않는데 이런 단어를 한국어로는 **불용어**, 영어로는 **stopword**라고 합니다. **Stop** 토큰 필터를 적용하면 불용어에 해당되는 텀 들을 제거합니다.

&#x20; `stopwords` 항목에 불용어로 지정할 단어들을 배열 형태로 나열하거나 `"_english_"`, `"_german_"` 같이 언어를 지정해서 해당 언어팩에 있는 불용어를 지정할 수도 있습니다. 지원되는 언어팩은 [공식 도큐먼트](https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-stop-tokenfilter.html)에서 확인할 수 있으며 한, 중, 일어 등은 별도의 형태소 분석기를 사용해야 합니다. 불용어 목록을 별도의 텍스트 파일로 저장하고 저장된 파일 경로를 `stopwords_path` 항목의 값으로 지정하여 사용하는 것도 가능합니다.

&#x20; 다음은 **my_stop** 인덱스에 **"in"**, **"the"**, **"days"** 를 불용어로 처리하는 **my_stop_filter** 라는 이름의 `stop` 토큰필터를 정의하고 `lowercase` 필터와 함께 **"Around the World in Eighty Days"** 문장을 분석 하는 예제입니다.

{% hint style="warning" %}
불용어로 처리할 단어들이 소문자이기 때문에 분석할 때는 반드시 lowercase 토큰필터를 먼저 적용해야 합니다.
{% endhint %}

{% code title="my_stop 인덱스에 my_stop_filter 토큰필터 생성" %}
```javascript
PUT my_stop
{
  "settings": {
    "analysis": {
      "filter": {
        "my_stop_filter": {
          "type": "stop",
          "stopwords": [
            "in",
            "the",
            "days"
          ]
        }
      }
    }
  }
}
```
{% endcode %}

{% tabs %}
{% tab title="request" %}
{% code title="my_stop_filter 토큰 필터로 문장 분석" %}
```javascript
GET my_stop/_analyze
{
  "tokenizer": "whitespace",
  "filter": [
    "lowercase",
    "my_stop_filter"
  ],
  "text": [ "Around the World in Eighty Days" ]
}
```
{% endcode %}
{% endtab %}

{% tab title="response" %}
{% code title="my_stop_filter 토큰 필터로 문장 분석 결과" %}
```javascript
{
  "tokens" : [
    {
      "token" : "around",
      "start_offset" : 0,
      "end_offset" : 6,
      "type" : "word",
      "position" : 0
    },
    {
      "token" : "world",
      "start_offset" : 11,
      "end_offset" : 16,
      "type" : "word",
      "position" : 2
    },
    {
      "token" : "eighty",
      "start_offset" : 20,
      "end_offset" : 26,
      "type" : "word",
      "position" : 4
    }
  ]
}
```
{% endcode %}
{% endtab %}
{% endtabs %}

&#x20; 이번에는 불용어 **"in"**, **"the"**, **"eighty"**를 **my_stop_dic.txt** 파일 안에 저장하고 이 파일을 읽어들여 동일한 문장을 분석 해 보는 예제입니다. 불용어는 모두 줄바꿈으로 입력해야 하며 사전 파일 경로는 **elasticsearch** 의 **config** 디렉토리를 기준으로 상대 경로를 지정해야 하며 텍스트 인코딩은 반드시 **UTF-8** 로 되어 있어야 합니다. **my_stop_dic.txt** 파일은 **elasticsearch** 홈 아래의 **config/user_dic** 디렉토리에 저장되었다고 가정하겠습니다.

{% code title="config/user_dic 디렉토리 생성 후 my_stop_dic.txt 파일 생성" %}
```bash

$ mkdir config/user_dic
$ echo 'in
the
eighty' > config/user_dic/my_stop_dic.txt

```
{% endcode %}

{% code title="stopwords_path 설정을 가진 my_stop_filter 토큰필터 생성" %}
```javascript
PUT my_stop
{
  "settings": {
    "analysis": {
      "filter": {
        "my_stop_filter": {
          "type": "stop",
          "stopwords_path": "user_dic/my_stop_dic.txt"
        }
      }
    }
  }
}
```
{% endcode %}

{% tabs %}
{% tab title="request" %}
{% code title="my_stop_filter 토큰 필터로 문장 분석" %}
```javascript
GET my_stop/_analyze
{
  "tokenizer": "whitespace",
  "filter": [
    "lowercase",
    "my_stop_filter"
  ],
  "text": [ "Around the World in Eighty Days" ]
}
```
{% endcode %}
{% endtab %}

{% tab title="response" %}
{% code title="my_stop_filter 토큰 필터로 문장 분석 결과" %}
```javascript
{
  "tokens" : [
    {
      "token" : "around",
      "start_offset" : 0,
      "end_offset" : 6,
      "type" : "word",
      "position" : 0
    },
    {
      "token" : "world",
      "start_offset" : 11,
      "end_offset" : 16,
      "type" : "word",
      "position" : 2
    },
    {
      "token" : "days",
      "start_offset" : 27,
      "end_offset" : 31,
      "type" : "word",
      "position" : 5
    }
  ]
}
```
{% endcode %}
{% endtab %}
{% endtabs %}

&#x20; **my_stop_dic.txt** 파일 내용인 **in**, **the**, **eight** 가 제거된 나머지 텀 들만 결과로 나타난 것을 확인할 수 있습니다.

{% hint style="danger" %}
기존의 사전 파일의 내용이 변경 된 경우 인덱스를 새로 고침을 해 주어야 토큰 필터가 새로 적용됩니다. 이것은 **stop** 외에도 뒤에 설명할 **synonym** 이나 **nori 한글 형태소 분석기** 사전에도 동일하게 적용됩니다. 새로 고침을 하는 방법은

**POST <인덱스명>/_close**\
**POST <인덱스명>/_open**

을 차례대로 실행 해 주면 됩니다. 인덱스가 close 된 중에는 색인이나 검색이 불가능 하게 되니 주의해야 합니다.

또한 애널라이저의 사전만 갱신되는 것이기 때문에 이미 색인된 도큐먼트들의 역 색인 내용은 변경되지 않습니다. 인덱스 새로 고침 이후에 색인되는 데이터들과 match 쿼리의 검색 등에만 적용이 됩니다. 기존 도큐먼트의 역 색인을 변경하려면 데이터를 모두 다시 재색인을 해야 합니다.
{% endhint %}
