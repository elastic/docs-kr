---
id: esG7-06-05-tokenizer
slug: /krEsguide7/esG7-06-05-tokenizer
title: "6.5 토크나이저 - Tokenizer"
description: "모든 문서에 대한 저작권은 Elastic 과 김종민(kimjmin@gmail.com) 에게 있으며 허가되지 않은 무단 복제나 배포 및 출판을 금지합니다. 본 문서의 내용 및 포함된 자료를 인용하고자 하는 경우 출처를 명시하고 게재된 주소를 김종민(kimjmin@gmail.com)에게 알려주시기 바랍니다."
date: 2022-01-25
tags: ['indexing', 'text', 'analysis', 'token', 'tokenizer']
---

데이터 색인 과정에서 검색 기능에 가장 큰 영향을 미치는 단계가 토크나이저 입니다. 데이터 분석 과정에서 토크나이저는 반드시 **한 개**만 사용이 가능하며 `tokenizer` 항목에 단일값으로 설정합니다. 이 책에서는 자주 사용되고 유용한 토크나이저들 위주로 설명하겠습니다.

토크나이저들 중 **NGram**, **Lowercase** 같은 토크나이저들은 대부분은 Standard 토크나이저에 같은 이름의 토큰 필터를 내장한 들입니다. 이 책에서 다루지 않는 토크나이저들은 공식 홈페이지의 도큐먼트를 확인하시기 바랍니다.

## 6.5.1 Standard, Letter, Whitespace

일반적으로 가장 많이 사용되고 기능이 유사하지만 분명히 다른 특징이 있는 **Standard**, **Letter**, **Whitespace** 3가지 토크나이저를 먼저 살펴보도록 하겠습니다. 각자 따로 설명하는 것 보다 동일한 문장이 위의 세 토큰 필터에서 어떻게 다르게 분리가 되는지를 살펴보면서 설명을 하겠습니다. 분석할 문장은 **"THE quick.brown_FOx jumped! @ 3.5 meters."** 입니다.

<DocTabs>
  <DocTab name="request">
```javascript
# standard 토크나이저로 문장 분석
GET _analyze
{
  "tokenizer": "standard",
  "text": "THE quick.brown_FOx jumped! @ 3.5 meters."
}
```
  </DocTab>
  <DocTab name="response">
```javascript
# standard 토크나이저로 문장 분석 결과
{
  "tokens" : [
    {
      "token" : "THE",
      "start_offset" : 0,
      "end_offset" : 3,
      "type" : "<ALPHANUM>",
      "position" : 0
    },
    {
      "token" : "quick.brown_FOx",
      "start_offset" : 4,
      "end_offset" : 19,
      "type" : "<ALPHANUM>",
      "position" : 1
    },
    {
      "token" : "jumped",
      "start_offset" : 20,
      "end_offset" : 26,
      "type" : "<ALPHANUM>",
      "position" : 2
    },
    {
      "token" : "3.5",
      "start_offset" : 30,
      "end_offset" : 33,
      "type" : "<NUM>",
      "position" : 3
    },
    {
      "token" : "meters",
      "start_offset" : 34,
      "end_offset" : 40,
      "type" : "<ALPHANUM>",
      "position" : 4
    }
  ]
}
```
  </DocTab>
</DocTabs>

<DocTabs>
  <DocTab name="request">
```javascript
# letter 토크나이저로 문장 분석
GET _analyze
{
  "tokenizer": "letter",
  "text": "THE quick.brown_FOx jumped! @ 3.5 meters."
}
```
  </DocTab>
  <DocTab name="response">
```javascript
# letter 토크나이저로 문장 분석 결과
{
  "tokens" : [
    {
      "token" : "THE",
      "start_offset" : 0,
      "end_offset" : 3,
      "type" : "word",
      "position" : 0
    },
    {
      "token" : "quick",
      "start_offset" : 4,
      "end_offset" : 9,
      "type" : "word",
      "position" : 1
    },
    {
      "token" : "brown",
      "start_offset" : 10,
      "end_offset" : 15,
      "type" : "word",
      "position" : 2
    },
    {
      "token" : "FOx",
      "start_offset" : 16,
      "end_offset" : 19,
      "type" : "word",
      "position" : 3
    },
    {
      "token" : "jumped",
      "start_offset" : 20,
      "end_offset" : 26,
      "type" : "word",
      "position" : 4
    },
    {
      "token" : "meters",
      "start_offset" : 34,
      "end_offset" : 40,
      "type" : "word",
      "position" : 5
    }
  ]
}
```
  </DocTab>
</DocTabs>

<DocTabs>
  <DocTab name="request">
```javascript
# whitespace 토크나이저로 문장 분석
GET _analyze
{
  "tokenizer": "whitespace",
  "text": "THE quick.brown_FOx jumped! @ 3.5 meters."
}
```
  </DocTab>
  <DocTab name="response">
```javascript
# whitespace 토크나이저로 문장 분석 결과
{
  "tokens" : [
    {
      "token" : "THE",
      "start_offset" : 0,
      "end_offset" : 3,
      "type" : "word",
      "position" : 0
    },
    {
      "token" : "quick.brown_FOx",
      "start_offset" : 4,
      "end_offset" : 19,
      "type" : "word",
      "position" : 1
    },
    {
      "token" : "jumped!",
      "start_offset" : 20,
      "end_offset" : 27,
      "type" : "word",
      "position" : 2
    },
    {
      "token" : "@",
      "start_offset" : 28,
      "end_offset" : 29,
      "type" : "word",
      "position" : 3
    },
    {
      "token" : "3.5",
      "start_offset" : 30,
      "end_offset" : 33,
      "type" : "word",
      "position" : 4
    },
    {
      "token" : "meters.",
      "start_offset" : 34,
      "end_offset" : 41,
      "type" : "word",
      "position" : 5
    }
  ]
}
```
  </DocTab>
</DocTabs>

앞 예제들의 response 탭을 열어 각각의 결과를 확인 해 보면 다음과 같습니다.

먼저 **Standard** 토크나이저는 공백으로 텀을 구분하면서 "@"과 같은 일부 특수문자를 제거합니다. "jumped!"의 느낌표, "meters."의 마침표 처럼 단어 끝에 있는 특수문자는 제거되지만 "quick.brown_FOx" 또는 "3.5" 처럼 중간에 있는 마침표나 밑줄 등은 제거되거나 분리되지 않는 것을 확인할 수 있습니다.

**Letter** 토크나이저는 알파벳을 제외한 모든 공백, 숫자, 기호들을 기준으로 텀을 분리합니다. "quick.brown_FOx" 같은 단어도 "quick", "brown", "FOx" 처럼 모두 분리된 것을 확인할 수 있습니다.

**Whitespace** 토크나이저는 스페이스, 탭, 그리고 줄바꿈 같은 공백만을 기준으로 텀을 분리합니다. 특수문자 "@" 그리고 "meters." 의 마지막에 있는 마침표도 사라지지 않고 그대로 남아있는 것을 확인할 수 있습니다.

3개의 토크나이저 중에 **Letter** 토크나이저의 경우 검색 범위가 넓어져서 원하지 않는 결과가 많이 나올 수 있고, 반대로 **Whitespace**의 경우 특수문자를 거르지 않기 때문에 정확하게 검색을 하지 않으면 검색 결과가 나오지 않을 수 있습니다. 따라서 보통은 **Standard** 토크나이저를 많이 사용합니다.

## 6.5.2 UAX URL Email

주로 사용되는 Standard 토크나이저도 `@`, `/` 같은 특수문자는 공백과 마찬가지로 제거하고 분리합니다. 그런데 요즘의 블로그 포스트나 신문기사 같은 텍스트 들에는 이메일 주소 또는 웹 URL 경로 등이 삽입되어 있는 경우가 상당히 많습니다. 이 경우 Standard 토크나이저를 사용하면 이메일 주소등이 정상적으로 인식되지 않아 문제가 될 수 있는데, 이를 방지하기 위해 사용 가능한 것이 **UAX URL Email** 토크나이저 입니다.

**UAX URL Email** 토크나이저는 이메일 주소와 웹 URL 경로는 분리하지 않고 그대로 하나의 텀으로 저장을 합니다. 다음은 **"email address is my-name@email.com and website is https://www.elastic.co"** 문장을 각각 Standard 그리고 UAX URL Email 토크나이저로 분리 한 결과입니다.

<DocTabs>
  <DocTab name="request">
```javascript
# standard 토크나이저로 문장 분석
GET _analyze
{
  "tokenizer": "standard",
  "text": "email address is my-name@email.com and website is https://www.elastic.co"
}
```
  </DocTab>
  <DocTab name="response">
```javascript
# standard 토크나이저로 문장 분석 결과
{
  "tokens" : [
    {
      "token" : "email",
      "start_offset" : 0,
      "end_offset" : 5,
      "type" : "<ALPHANUM>",
      "position" : 0
    },
    {
      "token" : "address",
      "start_offset" : 6,
      "end_offset" : 13,
      "type" : "<ALPHANUM>",
      "position" : 1
    },
    {
      "token" : "is",
      "start_offset" : 14,
      "end_offset" : 16,
      "type" : "<ALPHANUM>",
      "position" : 2
    },
    {
      "token" : "my",
      "start_offset" : 17,
      "end_offset" : 19,
      "type" : "<ALPHANUM>",
      "position" : 3
    },
    {
      "token" : "name",
      "start_offset" : 20,
      "end_offset" : 24,
      "type" : "<ALPHANUM>",
      "position" : 4
    },
    {
      "token" : "email.com",
      "start_offset" : 25,
      "end_offset" : 34,
      "type" : "<ALPHANUM>",
      "position" : 5
    },
    {
      "token" : "and",
      "start_offset" : 35,
      "end_offset" : 38,
      "type" : "<ALPHANUM>",
      "position" : 6
    },
    {
      "token" : "website",
      "start_offset" : 39,
      "end_offset" : 46,
      "type" : "<ALPHANUM>",
      "position" : 7
    },
    {
      "token" : "is",
      "start_offset" : 47,
      "end_offset" : 49,
      "type" : "<ALPHANUM>",
      "position" : 8
    },
    {
      "token" : "https",
      "start_offset" : 50,
      "end_offset" : 55,
      "type" : "<ALPHANUM>",
      "position" : 9
    },
    {
      "token" : "www.elastic.co",
      "start_offset" : 58,
      "end_offset" : 72,
      "type" : "<ALPHANUM>",
      "position" : 10
    }
  ]
}
```
  </DocTab>
</DocTabs>

<DocTabs>
  <DocTab name="request">
```javascript
# uax_url_email 토크나이저로 문장 분석
GET _analyze
{
  "tokenizer": "uax_url_email",
  "text": "email address is my-name@email.com and website is https://www.elastic.co"
}
```
  </DocTab>
  <DocTab name="response">
```javascript
# uax_url_email 토크나이저로 문장 분석 결과
{
  "tokens" : [
    {
      "token" : "email",
      "start_offset" : 0,
      "end_offset" : 5,
      "type" : "<ALPHANUM>",
      "position" : 0
    },
    {
      "token" : "address",
      "start_offset" : 6,
      "end_offset" : 13,
      "type" : "<ALPHANUM>",
      "position" : 1
    },
    {
      "token" : "is",
      "start_offset" : 14,
      "end_offset" : 16,
      "type" : "<ALPHANUM>",
      "position" : 2
    },
    {
      "token" : "my-name@email.com",
      "start_offset" : 17,
      "end_offset" : 34,
      "type" : "<EMAIL>",
      "position" : 3
    },
    {
      "token" : "and",
      "start_offset" : 35,
      "end_offset" : 38,
      "type" : "<ALPHANUM>",
      "position" : 4
    },
    {
      "token" : "website",
      "start_offset" : 39,
      "end_offset" : 46,
      "type" : "<ALPHANUM>",
      "position" : 5
    },
    {
      "token" : "is",
      "start_offset" : 47,
      "end_offset" : 49,
      "type" : "<ALPHANUM>",
      "position" : 6
    },
    {
      "token" : "https://www.elastic.co",
      "start_offset" : 50,
      "end_offset" : 72,
      "type" : "<URL>",
      "position" : 7
    }
  ]
}
```
  </DocTab>
</DocTabs>

 `"tokenizer": "uax_url_email"` 로 설정하여 텍스트를 분석하면 이메일 주소와 웹 URL은 그대로 남아있는 것을 확인할 수 있습니다.

## 6.5.3 Pattern

앞에서 살펴본 토크나이저들은 다소 차이는 있지만 기본적으로는 공백을 기준으로 하여 텀 들을 분리합니다. 분석 할 데이터가 사람이 읽는 일반적인 문장이 아니라 서버 시스템이나 IoT 장비 등에서 수집된 머신 데이터인 경우 공백이 아닌 쉼표나 세로선 같은 기호가 값 항목의 구분자로 사용되는 경우가 종종 있습니다. 이런 특수한 문자를 구분자로 사용하여 텀을 분리하고 싶은 경우 사용할 수 있는 것이 **Pattern** 토크나이저 입니다.

**Pattern** 토크나이저는 분리할 패턴을 기호 또는 Java 정규식 형태로 지정할 수 있습니다. 구분자 지정은 `pattern` 항목에 설정합니다. 다음은 인덱스 **pat_tokenizer**에 슬래시 `/`를 구분자로 하는 **my_pat_tokenizer**라는 사용자 정의 토크나이저를 만들고 **"/usr/share/elasticsearch/bin"** 를 분석하는 예제입니다.

```javascript
# pat_tokenizer 인덱스에 my_pat_tokenizer 토크나이저 생성
PUT pat_tokenizer
{
  "settings": {
    "analysis": {
      "tokenizer": {
        "my_pat_tokenizer": {
          "type": "pattern",
          "pattern": "/"
        }
      }
    }
  }
}
```

<DocTabs>
  <DocTab name="request">
```javascript
# my_pat_tokenizer 토크나이저로 문장 분석
GET pat_tokenizer/_analyze
{
  "tokenizer": "my_pat_tokenizer",
  "text": "/usr/share/elasticsearch/bin"
}
```
  </DocTab>
  <DocTab name="response">
```javascript
# my_pat_tokenizer 토크나이저로 문장 분석 결과
{
  "tokens" : [
    {
      "token" : "usr",
      "start_offset" : 1,
      "end_offset" : 4,
      "type" : "word",
      "position" : 0
    },
    {
      "token" : "share",
      "start_offset" : 5,
      "end_offset" : 10,
      "type" : "word",
      "position" : 1
    },
    {
      "token" : "elasticsearch",
      "start_offset" : 11,
      "end_offset" : 24,
      "type" : "word",
      "position" : 2
    },
    {
      "token" : "bin",
      "start_offset" : 25,
      "end_offset" : 28,
      "type" : "word",
      "position" : 3
    }
  ]
}
```
  </DocTab>
</DocTabs>

`"pattern": "/"` 같은 단일 기호 외에도 알파벳 대문자를 기준으로 텀을 분리하도록 하는 `"pattern": "(?<=\\p{Lower})(?=\\p{Upper})"` 와 같은 정규식(Regular Expression) 으로도 설정이 가능합니다.

## 6.5.4 Path Hierarchy

디렉토리나 파일 경로 등은 흔하게 저장되는 데이터입니다. 앞의 Pattern 토크나이저에서 **"/usr/share/elasticsearch/bin"** 를 실행했을 때는 디렉토리명 들이 각각 하나의 토큰으로 분리 된 것을 확인했습니다. 이 경우 다른 패스에 있는데 하위 디렉토리 명이 같은 경우 데이터 검색에 혼동이 올 수 있습니다.

**Path Hierarchy** 토크나이저를 사용하면 경로 데이터를 계층별로 저장해서 하위 디렉토리에 속한 도큐먼트들을 수준별로 검색하거나 집계하는 것이 가능합니다. 다음은 Path Hierarchy 토크나이저로 **"/usr/share/elasticsearch/bin"** 를 분석하는 예제입니다.

<DocTabs>
  <DocTab name="request">
```javascript
# path_hierarchy 토크나이저로 문장 분석
POST _analyze
{
  "tokenizer": "path_hierarchy",
  "text": "/usr/share/elasticsearch/bin"
}
```
  </DocTab>
  <DocTab name="response">
```javascript
# path_hierarchy 토크나이저로 문장 분석 결과
{
  "tokens" : [
    {
      "token" : "/usr",
      "start_offset" : 0,
      "end_offset" : 4,
      "type" : "word",
      "position" : 0
    },
    {
      "token" : "/usr/share",
      "start_offset" : 0,
      "end_offset" : 10,
      "type" : "word",
      "position" : 0
    },
    {
      "token" : "/usr/share/elasticsearch",
      "start_offset" : 0,
      "end_offset" : 24,
      "type" : "word",
      "position" : 0
    },
    {
      "token" : "/usr/share/elasticsearch/bin",
      "start_offset" : 0,
      "end_offset" : 28,
      "type" : "word",
      "position" : 0
    }
  ]
}
```
  </DocTab>
</DocTabs>

`delimiter` 항목값으로 경로 구분자를 지정할 수 있습니다. 디폴트는 `/` 입니다. 그리고 `replacement` 옵션을 이용해서 소스의 구분자를 다른 구분자로 대치해서 저장하는 것도 가능합니다. 그 외의 옵션들은 [공식 도큐먼트](https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-pathhierarchy-tokenizer.html)를 참고하시기 바랍니다.

다음은 인덱스 **hir_tokenizer**에 `-` 구분자를 `/` 구분자로 대치하는 **my_hir_tokenizer**라는 사용자 정의 토크나이저를 만들고 **"one-two-three"** 문장을 분석하는 예제입니다.

```javascript
# hir_tokenizer 인덱스에 my_hir_tokenizer 토크나이저 생성
PUT hir_tokenizer
{
  "settings": {
    "analysis": {
      "tokenizer": {
        "my_hir_tokenizer": {
          "type": "path_hierarchy",
          "delimiter": "-",
          "replacement": "/"
        }
      }
    }
  }
}
```

<DocTabs>
  <DocTab name="request">
```javascript
# hir_tokenizer 토크나이저로 문장 분석
GET hir_tokenizer/_analyze
{
  "tokenizer": "my_hir_tokenizer",
  "text": [
    "one-two-three"
  ]
}
```
  </DocTab>
  <DocTab name="response">
```javascript
# hir_tokenizer 토크나이저로 문장 분석 결과
{
  "tokens" : [
    {
      "token" : "one",
      "start_offset" : 0,
      "end_offset" : 3,
      "type" : "word",
      "position" : 0
    },
    {
      "token" : "one/two",
      "start_offset" : 0,
      "end_offset" : 7,
      "type" : "word",
      "position" : 0
    },
    {
      "token" : "one/two/three",
      "start_offset" : 0,
      "end_offset" : 13,
      "type" : "word",
      "position" : 0
    }
  ]
}
```
  </DocTab>
</DocTabs>