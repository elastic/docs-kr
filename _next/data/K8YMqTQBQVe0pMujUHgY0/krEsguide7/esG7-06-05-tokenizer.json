{"pageProps":{"bundle":"!function(e,n){\"object\"==typeof exports&&\"object\"==typeof module?module.exports=n(require(\"mdxJsReact\"),require(\"React\")):\"function\"==typeof define&&define.amd?define([\"mdxJsReact\",\"React\"],n):\"object\"==typeof exports?exports.MDXContent=n(require(\"mdxJsReact\"),require(\"React\")):e.MDXContent=n(e.mdxJsReact,e.React)}(this,((e,n)=>(()=>{\"use strict\";var t={24:e=>{e.exports=n},825:n=>{n.exports=e}},a={};function r(e){var n=a[e];if(void 0!==n)return n.exports;var o=a[e]={exports:{}};return t[e](o,o.exports,r),o.exports}r.n=e=>{var n=e&&e.__esModule?()=>e.default:()=>e;return r.d(n,{a:n}),n},r.d=(e,n)=>{for(var t in n)r.o(n,t)&&!r.o(e,t)&&Object.defineProperty(e,t,{enumerable:!0,get:n[t]})},r.o=(e,n)=>Object.prototype.hasOwnProperty.call(e,n),r.r=e=>{\"undefined\"!=typeof Symbol&&Symbol.toStringTag&&Object.defineProperty(e,Symbol.toStringTag,{value:\"Module\"}),Object.defineProperty(e,\"__esModule\",{value:!0})};var o={};return(()=>{r.r(o),r.d(o,{default:()=>a});var e=r(825),n=r(24),t=r.n(n);const a=function(){var n=arguments.length>0&&void 0!==arguments[0]?arguments[0]:{},a=Object.assign({},(0,e.useMDXComponents)(),n.components),r=a.wrapper;return r?t().createElement(r,n,t().createElement(o)):o();function o(){var a=Object.assign({p:\"p\",strong:\"strong\",code:\"code\",h2:\"h2\",pre:\"pre\",a:\"a\"},(0,e.useMDXComponents)(),n.components),r=a.DocTabs,o=a.DocTab;return o||s(\"DocTab\",!0),r||s(\"DocTabs\",!0),t().createElement(t().Fragment,null,t().createElement(a.p,null,\"데이터 색인 과정에서 검색 기능에 가장 큰 영향을 미치는 단계가 토크나이저 입니다. 데이터 분석 과정에서 토크나이저는 반드시 \",t().createElement(a.strong,null,\"한 개\"),\"만 사용이 가능하며 \",t().createElement(a.code,{display:\"inline\"},\"tokenizer\"),\" 항목에 단일값으로 설정합니다. 이 책에서는 자주 사용되고 유용한 토크나이저들 위주로 설명하겠습니다.\"),\"\\n\",t().createElement(a.p,null,\"토크나이저들 중 \",t().createElement(a.strong,null,\"NGram\"),\", \",t().createElement(a.strong,null,\"Lowercase\"),\" 같은 토크나이저들은 대부분은 Standard 토크나이저에 같은 이름의 토큰 필터를 내장한 들입니다. 이 책에서 다루지 않는 토크나이저들은 공식 홈페이지의 도큐먼트를 확인하시기 바랍니다.\"),\"\\n\",t().createElement(a.h2,{id:\"651-standard-letter-whitespace\"},\"6.5.1 Standard, Letter, Whitespace\"),\"\\n\",t().createElement(a.p,null,\"일반적으로 가장 많이 사용되고 기능이 유사하지만 분명히 다른 특징이 있는 \",t().createElement(a.strong,null,\"Standard\"),\", \",t().createElement(a.strong,null,\"Letter\"),\", \",t().createElement(a.strong,null,\"Whitespace\"),\" 3가지 토크나이저를 먼저 살펴보도록 하겠습니다. 각자 따로 설명하는 것 보다 동일한 문장이 위의 세 토큰 필터에서 어떻게 다르게 분리가 되는지를 살펴보면서 설명을 하겠습니다. 분석할 문장은 \",t().createElement(a.strong,null,'\"THE quick.brown_FOx jumped! @ 3.5 meters.\"'),\" 입니다.\"),\"\\n\",t().createElement(r,null,t().createElement(o,{name:\"request\"},t().createElement(a.pre,null,t().createElement(a.code,{className:\"language-javascript\",display:\"block\"},'# standard 토크나이저로 문장 분석\\nGET _analyze\\n{\\n  \"tokenizer\": \"standard\",\\n  \"text\": \"THE quick.brown_FOx jumped! @ 3.5 meters.\"\\n}\\n'))),t().createElement(o,{name:\"response\"},t().createElement(a.pre,null,t().createElement(a.code,{className:\"language-javascript\",display:\"block\"},'# standard 토크나이저로 문장 분석 결과\\n{\\n  \"tokens\" : [\\n    {\\n      \"token\" : \"THE\",\\n      \"start_offset\" : 0,\\n      \"end_offset\" : 3,\\n      \"type\" : \"<ALPHANUM>\",\\n      \"position\" : 0\\n    },\\n    {\\n      \"token\" : \"quick.brown_FOx\",\\n      \"start_offset\" : 4,\\n      \"end_offset\" : 19,\\n      \"type\" : \"<ALPHANUM>\",\\n      \"position\" : 1\\n    },\\n    {\\n      \"token\" : \"jumped\",\\n      \"start_offset\" : 20,\\n      \"end_offset\" : 26,\\n      \"type\" : \"<ALPHANUM>\",\\n      \"position\" : 2\\n    },\\n    {\\n      \"token\" : \"3.5\",\\n      \"start_offset\" : 30,\\n      \"end_offset\" : 33,\\n      \"type\" : \"<NUM>\",\\n      \"position\" : 3\\n    },\\n    {\\n      \"token\" : \"meters\",\\n      \"start_offset\" : 34,\\n      \"end_offset\" : 40,\\n      \"type\" : \"<ALPHANUM>\",\\n      \"position\" : 4\\n    }\\n  ]\\n}\\n')))),\"\\n\",t().createElement(r,null,t().createElement(o,{name:\"request\"},t().createElement(a.pre,null,t().createElement(a.code,{className:\"language-javascript\",display:\"block\"},'# letter 토크나이저로 문장 분석\\nGET _analyze\\n{\\n  \"tokenizer\": \"letter\",\\n  \"text\": \"THE quick.brown_FOx jumped! @ 3.5 meters.\"\\n}\\n'))),t().createElement(o,{name:\"response\"},t().createElement(a.pre,null,t().createElement(a.code,{className:\"language-javascript\",display:\"block\"},'# letter 토크나이저로 문장 분석 결과\\n{\\n  \"tokens\" : [\\n    {\\n      \"token\" : \"THE\",\\n      \"start_offset\" : 0,\\n      \"end_offset\" : 3,\\n      \"type\" : \"word\",\\n      \"position\" : 0\\n    },\\n    {\\n      \"token\" : \"quick\",\\n      \"start_offset\" : 4,\\n      \"end_offset\" : 9,\\n      \"type\" : \"word\",\\n      \"position\" : 1\\n    },\\n    {\\n      \"token\" : \"brown\",\\n      \"start_offset\" : 10,\\n      \"end_offset\" : 15,\\n      \"type\" : \"word\",\\n      \"position\" : 2\\n    },\\n    {\\n      \"token\" : \"FOx\",\\n      \"start_offset\" : 16,\\n      \"end_offset\" : 19,\\n      \"type\" : \"word\",\\n      \"position\" : 3\\n    },\\n    {\\n      \"token\" : \"jumped\",\\n      \"start_offset\" : 20,\\n      \"end_offset\" : 26,\\n      \"type\" : \"word\",\\n      \"position\" : 4\\n    },\\n    {\\n      \"token\" : \"meters\",\\n      \"start_offset\" : 34,\\n      \"end_offset\" : 40,\\n      \"type\" : \"word\",\\n      \"position\" : 5\\n    }\\n  ]\\n}\\n')))),\"\\n\",t().createElement(r,null,t().createElement(o,{name:\"request\"},t().createElement(a.pre,null,t().createElement(a.code,{className:\"language-javascript\",display:\"block\"},'# whitespace 토크나이저로 문장 분석\\nGET _analyze\\n{\\n  \"tokenizer\": \"whitespace\",\\n  \"text\": \"THE quick.brown_FOx jumped! @ 3.5 meters.\"\\n}\\n'))),t().createElement(o,{name:\"response\"},t().createElement(a.pre,null,t().createElement(a.code,{className:\"language-javascript\",display:\"block\"},'# whitespace 토크나이저로 문장 분석 결과\\n{\\n  \"tokens\" : [\\n    {\\n      \"token\" : \"THE\",\\n      \"start_offset\" : 0,\\n      \"end_offset\" : 3,\\n      \"type\" : \"word\",\\n      \"position\" : 0\\n    },\\n    {\\n      \"token\" : \"quick.brown_FOx\",\\n      \"start_offset\" : 4,\\n      \"end_offset\" : 19,\\n      \"type\" : \"word\",\\n      \"position\" : 1\\n    },\\n    {\\n      \"token\" : \"jumped!\",\\n      \"start_offset\" : 20,\\n      \"end_offset\" : 27,\\n      \"type\" : \"word\",\\n      \"position\" : 2\\n    },\\n    {\\n      \"token\" : \"@\",\\n      \"start_offset\" : 28,\\n      \"end_offset\" : 29,\\n      \"type\" : \"word\",\\n      \"position\" : 3\\n    },\\n    {\\n      \"token\" : \"3.5\",\\n      \"start_offset\" : 30,\\n      \"end_offset\" : 33,\\n      \"type\" : \"word\",\\n      \"position\" : 4\\n    },\\n    {\\n      \"token\" : \"meters.\",\\n      \"start_offset\" : 34,\\n      \"end_offset\" : 41,\\n      \"type\" : \"word\",\\n      \"position\" : 5\\n    }\\n  ]\\n}\\n')))),\"\\n\",t().createElement(a.p,null,\"앞 예제들의 response 탭을 열어 각각의 결과를 확인 해 보면 다음과 같습니다.\"),\"\\n\",t().createElement(a.p,null,\"먼저 \",t().createElement(a.strong,null,\"Standard\"),' 토크나이저는 공백으로 텀을 구분하면서 \"@\"과 같은 일부 특수문자를 제거합니다. \"jumped!\"의 느낌표, \"meters.\"의 마침표 처럼 단어 끝에 있는 특수문자는 제거되지만 \"quick.brown_FOx\" 또는 \"3.5\" 처럼 중간에 있는 마침표나 밑줄 등은 제거되거나 분리되지 않는 것을 확인할 수 있습니다.'),\"\\n\",t().createElement(a.p,null,t().createElement(a.strong,null,\"Letter\"),' 토크나이저는 알파벳을 제외한 모든 공백, 숫자, 기호들을 기준으로 텀을 분리합니다. \"quick.brown_FOx\" 같은 단어도 \"quick\", \"brown\", \"FOx\" 처럼 모두 분리된 것을 확인할 수 있습니다.'),\"\\n\",t().createElement(a.p,null,t().createElement(a.strong,null,\"Whitespace\"),' 토크나이저는 스페이스, 탭, 그리고 줄바꿈 같은 공백만을 기준으로 텀을 분리합니다. 특수문자 \"@\" 그리고 \"meters.\" 의 마지막에 있는 마침표도 사라지지 않고 그대로 남아있는 것을 확인할 수 있습니다.'),\"\\n\",t().createElement(a.p,null,\"3개의 토크나이저 중에 \",t().createElement(a.strong,null,\"Letter\"),\" 토크나이저의 경우 검색 범위가 넓어져서 원하지 않는 결과가 많이 나올 수 있고, 반대로 \",t().createElement(a.strong,null,\"Whitespace\"),\"의 경우 특수문자를 거르지 않기 때문에 정확하게 검색을 하지 않으면 검색 결과가 나오지 않을 수 있습니다. 따라서 보통은 \",t().createElement(a.strong,null,\"Standard\"),\" 토크나이저를 많이 사용합니다.\"),\"\\n\",t().createElement(a.h2,{id:\"652-uax-url-email\"},\"6.5.2 UAX URL Email\"),\"\\n\",t().createElement(a.p,null,\"주로 사용되는 Standard 토크나이저도 \",t().createElement(a.code,{display:\"inline\"},\"@\"),\", \",t().createElement(a.code,{display:\"inline\"},\"/\"),\" 같은 특수문자는 공백과 마찬가지로 제거하고 분리합니다. 그런데 요즘의 블로그 포스트나 신문기사 같은 텍스트 들에는 이메일 주소 또는 웹 URL 경로 등이 삽입되어 있는 경우가 상당히 많습니다. 이 경우 Standard 토크나이저를 사용하면 이메일 주소등이 정상적으로 인식되지 않아 문제가 될 수 있는데, 이를 방지하기 위해 사용 가능한 것이 \",t().createElement(a.strong,null,\"UAX URL Email\"),\" 토크나이저 입니다.\"),\"\\n\",t().createElement(a.p,null,t().createElement(a.strong,null,\"UAX URL Email\"),\" 토크나이저는 이메일 주소와 웹 URL 경로는 분리하지 않고 그대로 하나의 텀으로 저장을 합니다. 다음은 \",t().createElement(a.strong,null,'\"email address is ',t().createElement(a.a,{href:\"mailto:my-name@email.com\"},\"my-name@email.com\"),\" and website is \",t().createElement(a.a,{href:\"https://www.elastic.co\"},\"https://www.elastic.co\"),'\"'),\" 문장을 각각 Standard 그리고 UAX URL Email 토크나이저로 분리 한 결과입니다.\"),\"\\n\",t().createElement(r,null,t().createElement(o,{name:\"request\"},t().createElement(a.pre,null,t().createElement(a.code,{className:\"language-javascript\",display:\"block\"},'# standard 토크나이저로 문장 분석\\nGET _analyze\\n{\\n  \"tokenizer\": \"standard\",\\n  \"text\": \"email address is my-name@email.com and website is https://www.elastic.co\"\\n}\\n'))),t().createElement(o,{name:\"response\"},t().createElement(a.pre,null,t().createElement(a.code,{className:\"language-javascript\",display:\"block\"},'# standard 토크나이저로 문장 분석 결과\\n{\\n  \"tokens\" : [\\n    {\\n      \"token\" : \"email\",\\n      \"start_offset\" : 0,\\n      \"end_offset\" : 5,\\n      \"type\" : \"<ALPHANUM>\",\\n      \"position\" : 0\\n    },\\n    {\\n      \"token\" : \"address\",\\n      \"start_offset\" : 6,\\n      \"end_offset\" : 13,\\n      \"type\" : \"<ALPHANUM>\",\\n      \"position\" : 1\\n    },\\n    {\\n      \"token\" : \"is\",\\n      \"start_offset\" : 14,\\n      \"end_offset\" : 16,\\n      \"type\" : \"<ALPHANUM>\",\\n      \"position\" : 2\\n    },\\n    {\\n      \"token\" : \"my\",\\n      \"start_offset\" : 17,\\n      \"end_offset\" : 19,\\n      \"type\" : \"<ALPHANUM>\",\\n      \"position\" : 3\\n    },\\n    {\\n      \"token\" : \"name\",\\n      \"start_offset\" : 20,\\n      \"end_offset\" : 24,\\n      \"type\" : \"<ALPHANUM>\",\\n      \"position\" : 4\\n    },\\n    {\\n      \"token\" : \"email.com\",\\n      \"start_offset\" : 25,\\n      \"end_offset\" : 34,\\n      \"type\" : \"<ALPHANUM>\",\\n      \"position\" : 5\\n    },\\n    {\\n      \"token\" : \"and\",\\n      \"start_offset\" : 35,\\n      \"end_offset\" : 38,\\n      \"type\" : \"<ALPHANUM>\",\\n      \"position\" : 6\\n    },\\n    {\\n      \"token\" : \"website\",\\n      \"start_offset\" : 39,\\n      \"end_offset\" : 46,\\n      \"type\" : \"<ALPHANUM>\",\\n      \"position\" : 7\\n    },\\n    {\\n      \"token\" : \"is\",\\n      \"start_offset\" : 47,\\n      \"end_offset\" : 49,\\n      \"type\" : \"<ALPHANUM>\",\\n      \"position\" : 8\\n    },\\n    {\\n      \"token\" : \"https\",\\n      \"start_offset\" : 50,\\n      \"end_offset\" : 55,\\n      \"type\" : \"<ALPHANUM>\",\\n      \"position\" : 9\\n    },\\n    {\\n      \"token\" : \"www.elastic.co\",\\n      \"start_offset\" : 58,\\n      \"end_offset\" : 72,\\n      \"type\" : \"<ALPHANUM>\",\\n      \"position\" : 10\\n    }\\n  ]\\n}\\n')))),\"\\n\",t().createElement(r,null,t().createElement(o,{name:\"request\"},t().createElement(a.pre,null,t().createElement(a.code,{className:\"language-javascript\",display:\"block\"},'# uax_url_email 토크나이저로 문장 분석\\nGET _analyze\\n{\\n  \"tokenizer\": \"uax_url_email\",\\n  \"text\": \"email address is my-name@email.com and website is https://www.elastic.co\"\\n}\\n'))),t().createElement(o,{name:\"response\"},t().createElement(a.pre,null,t().createElement(a.code,{className:\"language-javascript\",display:\"block\"},'# uax_url_email 토크나이저로 문장 분석 결과\\n{\\n  \"tokens\" : [\\n    {\\n      \"token\" : \"email\",\\n      \"start_offset\" : 0,\\n      \"end_offset\" : 5,\\n      \"type\" : \"<ALPHANUM>\",\\n      \"position\" : 0\\n    },\\n    {\\n      \"token\" : \"address\",\\n      \"start_offset\" : 6,\\n      \"end_offset\" : 13,\\n      \"type\" : \"<ALPHANUM>\",\\n      \"position\" : 1\\n    },\\n    {\\n      \"token\" : \"is\",\\n      \"start_offset\" : 14,\\n      \"end_offset\" : 16,\\n      \"type\" : \"<ALPHANUM>\",\\n      \"position\" : 2\\n    },\\n    {\\n      \"token\" : \"my-name@email.com\",\\n      \"start_offset\" : 17,\\n      \"end_offset\" : 34,\\n      \"type\" : \"<EMAIL>\",\\n      \"position\" : 3\\n    },\\n    {\\n      \"token\" : \"and\",\\n      \"start_offset\" : 35,\\n      \"end_offset\" : 38,\\n      \"type\" : \"<ALPHANUM>\",\\n      \"position\" : 4\\n    },\\n    {\\n      \"token\" : \"website\",\\n      \"start_offset\" : 39,\\n      \"end_offset\" : 46,\\n      \"type\" : \"<ALPHANUM>\",\\n      \"position\" : 5\\n    },\\n    {\\n      \"token\" : \"is\",\\n      \"start_offset\" : 47,\\n      \"end_offset\" : 49,\\n      \"type\" : \"<ALPHANUM>\",\\n      \"position\" : 6\\n    },\\n    {\\n      \"token\" : \"https://www.elastic.co\",\\n      \"start_offset\" : 50,\\n      \"end_offset\" : 72,\\n      \"type\" : \"<URL>\",\\n      \"position\" : 7\\n    }\\n  ]\\n}\\n')))),\"\\n\",t().createElement(a.p,null,t().createElement(a.code,{display:\"inline\"},'\"tokenizer\": \"uax_url_email\"'),\" 로 설정하여 텍스트를 분석하면 이메일 주소와 웹 URL은 그대로 남아있는 것을 확인할 수 있습니다.\"),\"\\n\",t().createElement(a.h2,{id:\"653-pattern\"},\"6.5.3 Pattern\"),\"\\n\",t().createElement(a.p,null,\"앞에서 살펴본 토크나이저들은 다소 차이는 있지만 기본적으로는 공백을 기준으로 하여 텀 들을 분리합니다. 분석 할 데이터가 사람이 읽는 일반적인 문장이 아니라 서버 시스템이나 IoT 장비 등에서 수집된 머신 데이터인 경우 공백이 아닌 쉼표나 세로선 같은 기호가 값 항목의 구분자로 사용되는 경우가 종종 있습니다. 이런 특수한 문자를 구분자로 사용하여 텀을 분리하고 싶은 경우 사용할 수 있는 것이 \",t().createElement(a.strong,null,\"Pattern\"),\" 토크나이저 입니다.\"),\"\\n\",t().createElement(a.p,null,t().createElement(a.strong,null,\"Pattern\"),\" 토크나이저는 분리할 패턴을 기호 또는 Java 정규식 형태로 지정할 수 있습니다. 구분자 지정은 \",t().createElement(a.code,{display:\"inline\"},\"pattern\"),\" 항목에 설정합니다. 다음은 인덱스 \",t().createElement(a.strong,null,\"pat_tokenizer\"),\"에 슬래시 \",t().createElement(a.code,{display:\"inline\"},\"/\"),\"를 구분자로 하는 \",t().createElement(a.strong,null,\"my_pat_tokenizer\"),\"라는 사용자 정의 토크나이저를 만들고 \",t().createElement(a.strong,null,'\"/usr/share/elasticsearch/bin\"'),\" 를 분석하는 예제입니다.\"),\"\\n\",t().createElement(a.pre,null,t().createElement(a.code,{className:\"language-javascript\",display:\"block\"},'# pat_tokenizer 인덱스에 my_pat_tokenizer 토크나이저 생성\\nPUT pat_tokenizer\\n{\\n  \"settings\": {\\n    \"analysis\": {\\n      \"tokenizer\": {\\n        \"my_pat_tokenizer\": {\\n          \"type\": \"pattern\",\\n          \"pattern\": \"/\"\\n        }\\n      }\\n    }\\n  }\\n}\\n')),\"\\n\",t().createElement(r,null,t().createElement(o,{name:\"request\"},t().createElement(a.pre,null,t().createElement(a.code,{className:\"language-javascript\",display:\"block\"},'# my_pat_tokenizer 토크나이저로 문장 분석\\nGET pat_tokenizer/_analyze\\n{\\n  \"tokenizer\": \"my_pat_tokenizer\",\\n  \"text\": \"/usr/share/elasticsearch/bin\"\\n}\\n'))),t().createElement(o,{name:\"response\"},t().createElement(a.pre,null,t().createElement(a.code,{className:\"language-javascript\",display:\"block\"},'# my_pat_tokenizer 토크나이저로 문장 분석 결과\\n{\\n  \"tokens\" : [\\n    {\\n      \"token\" : \"usr\",\\n      \"start_offset\" : 1,\\n      \"end_offset\" : 4,\\n      \"type\" : \"word\",\\n      \"position\" : 0\\n    },\\n    {\\n      \"token\" : \"share\",\\n      \"start_offset\" : 5,\\n      \"end_offset\" : 10,\\n      \"type\" : \"word\",\\n      \"position\" : 1\\n    },\\n    {\\n      \"token\" : \"elasticsearch\",\\n      \"start_offset\" : 11,\\n      \"end_offset\" : 24,\\n      \"type\" : \"word\",\\n      \"position\" : 2\\n    },\\n    {\\n      \"token\" : \"bin\",\\n      \"start_offset\" : 25,\\n      \"end_offset\" : 28,\\n      \"type\" : \"word\",\\n      \"position\" : 3\\n    }\\n  ]\\n}\\n')))),\"\\n\",t().createElement(a.p,null,t().createElement(a.code,{display:\"inline\"},'\"pattern\": \"/\"'),\" 같은 단일 기호 외에도 알파벳 대문자를 기준으로 텀을 분리하도록 하는 \",t().createElement(a.code,{display:\"inline\"},'\"pattern\": \"(?<=\\\\\\\\p{Lower})(?=\\\\\\\\p{Upper})\"'),\" 와 같은 정규식(Regular Expression) 으로도 설정이 가능합니다.\"),\"\\n\",t().createElement(a.h2,{id:\"654-path-hierarchy\"},\"6.5.4 Path Hierarchy\"),\"\\n\",t().createElement(a.p,null,\"디렉토리나 파일 경로 등은 흔하게 저장되는 데이터입니다. 앞의 Pattern 토크나이저에서 \",t().createElement(a.strong,null,'\"/usr/share/elasticsearch/bin\"'),\" 를 실행했을 때는 디렉토리명 들이 각각 하나의 토큰으로 분리 된 것을 확인했습니다. 이 경우 다른 패스에 있는데 하위 디렉토리 명이 같은 경우 데이터 검색에 혼동이 올 수 있습니다.\"),\"\\n\",t().createElement(a.p,null,t().createElement(a.strong,null,\"Path Hierarchy\"),\" 토크나이저를 사용하면 경로 데이터를 계층별로 저장해서 하위 디렉토리에 속한 도큐먼트들을 수준별로 검색하거나 집계하는 것이 가능합니다. 다음은 Path Hierarchy 토크나이저로 \",t().createElement(a.strong,null,'\"/usr/share/elasticsearch/bin\"'),\" 를 분석하는 예제입니다.\"),\"\\n\",t().createElement(r,null,t().createElement(o,{name:\"request\"},t().createElement(a.pre,null,t().createElement(a.code,{className:\"language-javascript\",display:\"block\"},'# path_hierarchy 토크나이저로 문장 분석\\nPOST _analyze\\n{\\n  \"tokenizer\": \"path_hierarchy\",\\n  \"text\": \"/usr/share/elasticsearch/bin\"\\n}\\n'))),t().createElement(o,{name:\"response\"},t().createElement(a.pre,null,t().createElement(a.code,{className:\"language-javascript\",display:\"block\"},'# path_hierarchy 토크나이저로 문장 분석 결과\\n{\\n  \"tokens\" : [\\n    {\\n      \"token\" : \"/usr\",\\n      \"start_offset\" : 0,\\n      \"end_offset\" : 4,\\n      \"type\" : \"word\",\\n      \"position\" : 0\\n    },\\n    {\\n      \"token\" : \"/usr/share\",\\n      \"start_offset\" : 0,\\n      \"end_offset\" : 10,\\n      \"type\" : \"word\",\\n      \"position\" : 0\\n    },\\n    {\\n      \"token\" : \"/usr/share/elasticsearch\",\\n      \"start_offset\" : 0,\\n      \"end_offset\" : 24,\\n      \"type\" : \"word\",\\n      \"position\" : 0\\n    },\\n    {\\n      \"token\" : \"/usr/share/elasticsearch/bin\",\\n      \"start_offset\" : 0,\\n      \"end_offset\" : 28,\\n      \"type\" : \"word\",\\n      \"position\" : 0\\n    }\\n  ]\\n}\\n')))),\"\\n\",t().createElement(a.p,null,t().createElement(a.code,{display:\"inline\"},\"delimiter\"),\" 항목값으로 경로 구분자를 지정할 수 있습니다. 디폴트는 \",t().createElement(a.code,{display:\"inline\"},\"/\"),\" 입니다. 그리고 \",t().createElement(a.code,{display:\"inline\"},\"replacement\"),\" 옵션을 이용해서 소스의 구분자를 다른 구분자로 대치해서 저장하는 것도 가능합니다. 그 외의 옵션들은 \",t().createElement(a.a,{href:\"https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-pathhierarchy-tokenizer.html\"},\"공식 도큐먼트\"),\"를 참고하시기 바랍니다.\"),\"\\n\",t().createElement(a.p,null,\"다음은 인덱스 \",t().createElement(a.strong,null,\"hir_tokenizer\"),\"에 \",t().createElement(a.code,{display:\"inline\"},\"-\"),\" 구분자를 \",t().createElement(a.code,{display:\"inline\"},\"/\"),\" 구분자로 대치하는 \",t().createElement(a.strong,null,\"my_hir_tokenizer\"),\"라는 사용자 정의 토크나이저를 만들고 \",t().createElement(a.strong,null,'\"one-two-three\"'),\" 문장을 분석하는 예제입니다.\"),\"\\n\",t().createElement(a.pre,null,t().createElement(a.code,{className:\"language-javascript\",display:\"block\"},'# hir_tokenizer 인덱스에 my_hir_tokenizer 토크나이저 생성\\nPUT hir_tokenizer\\n{\\n  \"settings\": {\\n    \"analysis\": {\\n      \"tokenizer\": {\\n        \"my_hir_tokenizer\": {\\n          \"type\": \"path_hierarchy\",\\n          \"delimiter\": \"-\",\\n          \"replacement\": \"/\"\\n        }\\n      }\\n    }\\n  }\\n}\\n')),\"\\n\",t().createElement(r,null,t().createElement(o,{name:\"request\"},t().createElement(a.pre,null,t().createElement(a.code,{className:\"language-javascript\",display:\"block\"},'# hir_tokenizer 토크나이저로 문장 분석\\nGET hir_tokenizer/_analyze\\n{\\n  \"tokenizer\": \"my_hir_tokenizer\",\\n  \"text\": [\\n    \"one-two-three\"\\n  ]\\n}\\n'))),t().createElement(o,{name:\"response\"},t().createElement(a.pre,null,t().createElement(a.code,{className:\"language-javascript\",display:\"block\"},'# hir_tokenizer 토크나이저로 문장 분석 결과\\n{\\n  \"tokens\" : [\\n    {\\n      \"token\" : \"one\",\\n      \"start_offset\" : 0,\\n      \"end_offset\" : 3,\\n      \"type\" : \"word\",\\n      \"position\" : 0\\n    },\\n    {\\n      \"token\" : \"one/two\",\\n      \"start_offset\" : 0,\\n      \"end_offset\" : 7,\\n      \"type\" : \"word\",\\n      \"position\" : 0\\n    },\\n    {\\n      \"token\" : \"one/two/three\",\\n      \"start_offset\" : 0,\\n      \"end_offset\" : 13,\\n      \"type\" : \"word\",\\n      \"position\" : 0\\n    }\\n  ]\\n}\\n')))))}};function s(e,n){throw new Error(\"Expected \"+(n?\"component\":\"object\")+\" `\"+e+\"` to be defined: you likely forgot to import, pass, or provide it.\")}})(),o})()));","frontmatter":{"id":"esG7-06-05-tokenizer","slug":"/krEsguide7/esG7-06-05-tokenizer","title":"6.5 토크나이저 - Tokenizer","description":"모든 문서에 대한 저작권은 Elastic 과 김종민(kimjmin@gmail.com) 에게 있으며 허가되지 않은 무단 복제나 배포 및 출판을 금지합니다. 본 문서의 내용 및 포함된 자료를 인용하고자 하는 경우 출처를 명시하고 게재된 주소를 김종민(kimjmin@gmail.com)에게 알려주시기 바랍니다.","date":"2022-01-25T00:00:00.000Z","tags":["indexing","text","analysis","token","tokenizer"],"link":"file:///Users/kimjmin/git/docs-kr/esguide7","linkPath":"06-text-analysis/06-05-tokenizer.mdx"},"missionId":"krEsguide7"},"__N_SSG":true}