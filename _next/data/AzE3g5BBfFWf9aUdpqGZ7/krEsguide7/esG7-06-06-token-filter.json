{"pageProps":{"bundle":"!function(e,n){\"object\"==typeof exports&&\"object\"==typeof module?module.exports=n(require(\"mdxJsReact\"),require(\"React\")):\"function\"==typeof define&&define.amd?define([\"mdxJsReact\",\"React\"],n):\"object\"==typeof exports?exports.MDXContent=n(require(\"mdxJsReact\"),require(\"React\")):e.MDXContent=n(e.mdxJsReact,e.React)}(this,((e,n)=>(()=>{\"use strict\";var t={24:e=>{e.exports=n},825:n=>{n.exports=e}},l={};function a(e){var n=l[e];if(void 0!==n)return n.exports;var s=l[e]={exports:{}};return t[e](s,s.exports,a),s.exports}a.n=e=>{var n=e&&e.__esModule?()=>e.default:()=>e;return a.d(n,{a:n}),n},a.d=(e,n)=>{for(var t in n)a.o(n,t)&&!a.o(e,t)&&Object.defineProperty(e,t,{enumerable:!0,get:n[t]})},a.o=(e,n)=>Object.prototype.hasOwnProperty.call(e,n),a.r=e=>{\"undefined\"!=typeof Symbol&&Symbol.toStringTag&&Object.defineProperty(e,Symbol.toStringTag,{value:\"Module\"}),Object.defineProperty(e,\"__esModule\",{value:!0})};var s={};return(()=>{a.r(s),a.d(s,{default:()=>l});var e=a(825),n=a(24),t=a.n(n);const l=function(){var n=arguments.length>0&&void 0!==arguments[0]?arguments[0]:{},l=Object.assign({},(0,e.useMDXComponents)(),n.components),a=l.wrapper;return a?t().createElement(a,n,t().createElement(s)):s();function s(){var l=Object.assign({p:\"p\",strong:\"strong\",code:\"code\",em:\"em\",a:\"a\",h2:\"h2\",pre:\"pre\",ul:\"ul\",li:\"li\",h3:\"h3\",img:\"img\"},(0,e.useMDXComponents)(),n.components),a=l.DocTabs,s=l.DocTab,o=l.DocCallOut;return o||r(\"DocCallOut\",!0),s||r(\"DocTab\",!0),a||r(\"DocTabs\",!0),t().createElement(t().Fragment,null,t().createElement(l.p,null,\"토크나이저를 이용한 텀 분리 과정 이후에는 분리된 각각의 텀 들을 지정한 규칙에 따라 처리를 해 주는데 이 과정을 담당하는 것이 \",t().createElement(l.strong,null,\"토큰 필터\"),\"입니다. 토큰 필터는 \",t().createElement(l.code,{display:\"inline\"},\"filter\"),\" \",t().createElement(l.em,null,t().createElement(l.strong,null,\"(token_filter가 아닙니다!)\")),\" 항목에 배열 값으로 나열해서 지정합니다. 하나만 사용하더라도 배열 값으로 입력해야 하며 나열된 순서대로 처리되기 때문에 순서를 잘 고려해서 입력해야 합니다.\"),\"\\n\",t().createElement(l.p,null,\"토큰 필터 역시 종류가 상당히 많고 계속 업데이트 되기 때문에 이 책에서는 자주 사용되는 토큰 필터 위주로 살펴보겠습니다. \",t().createElement(l.a,{href:\"https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-tokenfilters.html\"},\"공식 도큐먼트\"),\"에서 내가 필요로 하는 토큰 필터가 있는지 종종 들러서 살펴보시기 바랍니다.\"),\"\\n\",t().createElement(l.h2,{id:\"661-lowercase-uppercase\"},\"6.6.1 Lowercase, Uppercase\"),\"\\n\",t().createElement(l.p,null,\"영어나 유럽어 기반의 텍스트는 대소문자가 있어 검색할 때는 대소문자에 상관 없이검색이 가능하도록 처리 해 주어야 합니다. 보통은 텀 들을 모두 소문자로 변경하여 저장하는데 이 역할을 하는 것이 \",t().createElement(l.strong,null,\"Lowercase\"),\" 토큰 필터입니다. \",t().createElement(l.strong,null,\"Lowercase\"),\" 토큰 필터는 거의 모든 텍스트 검색 사례에서 사용되는 토큰 필터입니다.\"),\"\\n\",t().createElement(l.p,null,t().createElement(l.strong,null,\"Uppercase\"),\" 토큰 필터는 모든 텀을 대문자로 변경하는 것 이며 Lowercase 와 동일하게 설정합니다. 다음은 \",t().createElement(l.strong,null,'\"Harry Potter and the Philosopher\\'s Stone\"'),\" 문장을 lowercase와 uppercase 로 분석한 예제입니다.\"),\"\\n\",t().createElement(a,null,t().createElement(s,{name:\"request\"},t().createElement(l.pre,null,t().createElement(l.code,{className:\"language-javascript\",display:\"block\"},'# lowercase 토큰 필터로 문장 분석\\nGET _analyze\\n{\\n  \"filter\": [ \"lowercase\" ],\\n  \"text\": [ \"Harry Potter and the Philosopher\\'s Stone\" ]\\n}\\n'))),t().createElement(s,{name:\"response\"},t().createElement(l.pre,null,t().createElement(l.code,{className:\"language-javascript\",display:\"block\"},'# lowercase 토큰 필터로 문장 분석 결과\\n{\\n  \"tokens\" : [\\n    {\\n      \"token\" : \"harry potter and the philosopher\\'s stone\",\\n      \"start_offset\" : 0,\\n      \"end_offset\" : 40,\\n      \"type\" : \"word\",\\n      \"position\" : 0\\n    }\\n  ]\\n}\\n')))),\"\\n\",t().createElement(a,null,t().createElement(s,{name:\"request\"},t().createElement(l.pre,null,t().createElement(l.code,{className:\"language-javascript\",display:\"block\"},'# uppercase 토큰 필터로 문장 분석\\nGET _analyze\\n{\\n  \"filter\": [ \"uppercase\" ],\\n  \"text\": [ \"Harry Potter and the Philosopher\\'s Stone\" ]\\n}\\n'))),t().createElement(s,{name:\"response\"},t().createElement(l.pre,null,t().createElement(l.code,{className:\"language-javascript\",display:\"block\"},'# uppercase 토큰 필터로 문장 분석 결과\\n{\\n  \"tokens\" : [\\n    {\\n      \"token\" : \"HARRY POTTER AND THE PHILOSOPHER\\'S STONE\",\\n      \"start_offset\" : 0,\\n      \"end_offset\" : 40,\\n      \"type\" : \"word\",\\n      \"position\" : 0\\n    }\\n  ]\\n}\\n')))),\"\\n\",t().createElement(l.h2,{id:\"662-stop\"},\"6.6.2 Stop\"),\"\\n\",t().createElement(l.p,null,\"블로그 포스트나 뉴스 기사 같은 글에는 검색에서는 큰 의미가 없는 조사나 전치사 등이 많습니다. 영문에서도 \",t().createElement(l.strong,null,\"the\"),\", \",t().createElement(l.strong,null,\"is\"),\", \",t().createElement(l.strong,null,\"a\"),\" 같은 단어들은 대부분 검색어로 쓰이지 않는데 이런 단어를 한국어로는 \",t().createElement(l.strong,null,\"불용어\"),\", 영어로는 \",t().createElement(l.strong,null,\"stopword\"),\"라고 합니다. \",t().createElement(l.strong,null,\"Stop\"),\" 토큰 필터를 적용하면 불용어에 해당되는 텀 들을 제거합니다.\"),\"\\n\",t().createElement(l.p,null,t().createElement(l.code,{display:\"inline\"},\"stopwords\"),\" 항목에 불용어로 지정할 단어들을 배열 형태로 나열하거나 \",t().createElement(l.code,{display:\"inline\"},'\"_english_\"'),\", \",t().createElement(l.code,{display:\"inline\"},'\"_german_\"'),\" 같이 언어를 지정해서 해당 언어팩에 있는 불용어를 지정할 수도 있습니다. 지원되는 언어팩은 \",t().createElement(l.a,{href:\"https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-stop-tokenfilter.html\"},\"공식 도큐먼트\"),\"에서 확인할 수 있으며 한, 중, 일어 등은 별도의 형태소 분석기를 사용해야 합니다. 불용어 목록을 별도의 텍스트 파일로 저장하고 저장된 파일 경로를 \",t().createElement(l.code,{display:\"inline\"},\"stopwords_path\"),\" 항목의 값으로 지정하여 사용하는 것도 가능합니다.\"),\"\\n\",t().createElement(l.p,null,\"다음은 \",t().createElement(l.strong,null,\"my_stop\"),\" 인덱스에 \",t().createElement(l.strong,null,'\"in\"'),\", \",t().createElement(l.strong,null,'\"the\"'),\", \",t().createElement(l.strong,null,'\"days\"'),\" 를 불용어로 처리하는 \",t().createElement(l.strong,null,\"my_stop_filter\"),\" 라는 이름의 \",t().createElement(l.code,{display:\"inline\"},\"stop\"),\" 토큰필터를 정의하고 \",t().createElement(l.code,{display:\"inline\"},\"lowercase\"),\" 필터와 함께 \",t().createElement(l.strong,null,'\"Around the World in Eighty Days\"'),\" 문장을 분석 하는 예제입니다.\"),\"\\n\",t().createElement(o,{color:\"warning\"},t().createElement(l.p,null,\"불용어로 처리할 단어들이 소문자이기 때문에 분석할 때는 반드시 lowercase 토큰필터를 먼저 적용해야 합니다.\")),\"\\n\",t().createElement(l.pre,null,t().createElement(l.code,{className:\"language-javascript\",display:\"block\"},'# my_stop 인덱스에 my_stop_filter 토큰필터 생성\\nPUT my_stop\\n{\\n  \"settings\": {\\n    \"analysis\": {\\n      \"filter\": {\\n        \"my_stop_filter\": {\\n          \"type\": \"stop\",\\n          \"stopwords\": [\\n            \"in\",\\n            \"the\",\\n            \"days\"\\n          ]\\n        }\\n      }\\n    }\\n  }\\n}\\n')),\"\\n\",t().createElement(a,null,t().createElement(s,{name:\"request\"},t().createElement(l.pre,null,t().createElement(l.code,{className:\"language-javascript\",display:\"block\"},'# my_stop_filter 토큰 필터로 문장 분석\\nGET my_stop/_analyze\\n{\\n  \"tokenizer\": \"whitespace\",\\n  \"filter\": [\\n    \"lowercase\",\\n    \"my_stop_filter\"\\n  ],\\n  \"text\": [ \"Around the World in Eighty Days\" ]\\n}\\n'))),t().createElement(s,{name:\"response\"},t().createElement(l.pre,null,t().createElement(l.code,{className:\"language-javascript\",display:\"block\"},'# my_stop_filter 토큰 필터로 문장 분석 결과\\n{\\n  \"tokens\" : [\\n    {\\n      \"token\" : \"around\",\\n      \"start_offset\" : 0,\\n      \"end_offset\" : 6,\\n      \"type\" : \"word\",\\n      \"position\" : 0\\n    },\\n    {\\n      \"token\" : \"world\",\\n      \"start_offset\" : 11,\\n      \"end_offset\" : 16,\\n      \"type\" : \"word\",\\n      \"position\" : 2\\n    },\\n    {\\n      \"token\" : \"eighty\",\\n      \"start_offset\" : 20,\\n      \"end_offset\" : 26,\\n      \"type\" : \"word\",\\n      \"position\" : 4\\n    }\\n  ]\\n}\\n')))),\"\\n\",t().createElement(l.p,null,\"이번에는 불용어 \",t().createElement(l.strong,null,'\"in\"'),\", \",t().createElement(l.strong,null,'\"the\"'),', **\"eighty\"**를 ',t().createElement(l.strong,null,\"my_stop_dic.txt\"),\" 파일 안에 저장하고 이 파일을 읽어들여 동일한 문장을 분석 해 보는 예제입니다. 불용어는 모두 줄바꿈으로 입력해야 하며 사전 파일 경로는 \",t().createElement(l.strong,null,\"elasticsearch\"),\" 의 \",t().createElement(l.strong,null,\"config\"),\" 디렉토리를 기준으로 상대 경로를 지정해야 하며 텍스트 인코딩은 반드시 \",t().createElement(l.strong,null,\"UTF-8\"),\" 로 되어 있어야 합니다. \",t().createElement(l.strong,null,\"my_stop_dic.txt\"),\" 파일은 \",t().createElement(l.strong,null,\"elasticsearch\"),\" 홈 아래의 \",t().createElement(l.strong,null,\"config/user_dic\"),\" 디렉토리에 저장되었다고 가정하겠습니다.\"),\"\\n\",t().createElement(l.pre,null,t().createElement(l.code,{className:\"language-bash\",display:\"block\"},\"# config/user_dic 디렉토리 생성 후 my_stop_dic.txt 파일 생성\\n\\n$ mkdir config/user_dic\\n$ echo 'in\\nthe\\neighty' > config/user_dic/my_stop_dic.txt\\n\")),\"\\n\",t().createElement(l.pre,null,t().createElement(l.code,{className:\"language-javascript\",display:\"block\"},'# stopwords_path 설정을 가진 my_stop_filter 토큰필터 생성\\nPUT my_stop\\n{\\n  \"settings\": {\\n    \"analysis\": {\\n      \"filter\": {\\n        \"my_stop_filter\": {\\n          \"type\": \"stop\",\\n          \"stopwords_path\": \"user_dic/my_stop_dic.txt\"\\n        }\\n      }\\n    }\\n  }\\n}\\n')),\"\\n\",t().createElement(a,null,t().createElement(s,{name:\"request\"},t().createElement(l.pre,null,t().createElement(l.code,{className:\"language-javascript\",display:\"block\"},'# my_stop_filter 토큰 필터로 문장 분석\\nGET my_stop/_analyze\\n{\\n  \"tokenizer\": \"whitespace\",\\n  \"filter\": [\\n    \"lowercase\",\\n    \"my_stop_filter\"\\n  ],\\n  \"text\": [ \"Around the World in Eighty Days\" ]\\n}\\n'))),t().createElement(s,{name:\"response\"},t().createElement(l.pre,null,t().createElement(l.code,{className:\"language-javascript\",display:\"block\"},'# my_stop_filter 토큰 필터로 문장 분석 결과\\n{\\n  \"tokens\" : [\\n    {\\n      \"token\" : \"around\",\\n      \"start_offset\" : 0,\\n      \"end_offset\" : 6,\\n      \"type\" : \"word\",\\n      \"position\" : 0\\n    },\\n    {\\n      \"token\" : \"world\",\\n      \"start_offset\" : 11,\\n      \"end_offset\" : 16,\\n      \"type\" : \"word\",\\n      \"position\" : 2\\n    },\\n    {\\n      \"token\" : \"days\",\\n      \"start_offset\" : 27,\\n      \"end_offset\" : 31,\\n      \"type\" : \"word\",\\n      \"position\" : 5\\n    }\\n  ]\\n}\\n')))),\"\\n\",t().createElement(l.p,null,t().createElement(l.strong,null,\"my_stop_dic.txt\"),\" 파일 내용인 \",t().createElement(l.strong,null,\"in\"),\", \",t().createElement(l.strong,null,\"the\"),\", \",t().createElement(l.strong,null,\"eight\"),\" 가 제거된 나머지 텀 들만 결과로 나타난 것을 확인할 수 있습니다.\"),\"\\n\",t().createElement(o,{color:\"danger\"},t().createElement(l.p,null,\"기존의 사전 파일의 내용이 변경 된 경우 인덱스를 새로 고침을 해 주어야 토큰 필터가 새로 적용됩니다. 이것은 \",t().createElement(l.strong,null,\"stop\"),\" 외에도 뒤에 설명할 \",t().createElement(l.strong,null,\"synonym\"),\" 이나 \",t().createElement(l.strong,null,\"nori 한글 형태소 분석기\"),\" 사전에도 동일하게 적용됩니다. 새로 고침을 하는 방법은\"),t().createElement(l.p,null,t().createElement(l.strong,null,\"POST <인덱스명>/_close\"),\"\\n\",t().createElement(l.strong,null,\"POST <인덱스명>/_open\")),t().createElement(l.p,null,\"을 차례대로 실행 해 주면 됩니다. 인덱스가 close 된 중에는 색인이나 검색이 불가능 하게 되니 주의해야 합니다.\"),t().createElement(l.p,null,\"또한 애널라이저의 사전만 갱신되는 것이기 때문에 이미 색인된 도큐먼트들의 역 색인 내용은 변경되지 않습니다. 인덱스 새로 고침 이후에 색인되는 데이터들과 match 쿼리의 검색 등에만 적용이 됩니다. 기존 도큐먼트의 역 색인을 변경하려면 데이터를 모두 다시 재색인을 해야 합니다.\")),\"\\n\",t().createElement(l.h2,{id:\"663-synonym\"},\"6.6.3 Synonym\"),\"\\n\",t().createElement(l.p,null,\"검색 서비스에 따라서 \",t().createElement(l.strong,null,\"동의어\"),' 검색을 제공해야 하는 경우가 있습니다. 예를 들면 클라우드 서비스 관련 정보를 검색하는 시스템에서 \"AWS\" 라는 단어를 검색했을 때 \"Amazon\" 또는 한글 \"아마존\" 도 같이 검색을 하도록 하면 관련된 정보를 더 많이 찾을 수 있을 것입니다. 이 때 ',t().createElement(l.strong,null,\"Synonym\"),\" 토큰 필터를 사용하면 텀의 동의어 저장이 가능합니다. \"),\"\\n\",t().createElement(l.p,null,\"동의어를 설정하는 옵션은 \",t().createElement(l.code,{display:\"inline\"},\"synonyms\"),\" 항목에서 직접 동의어 목록을 입력하는 방법과 동의어 사전 파일을 만들어 \",t().createElement(l.code,{display:\"inline\"},\"synonyms_path\"),\" 로 지정하는 방법이 있습니다. 동의어 사전 명시 규칙에는 다음의 것들이 있습니다.\"),\"\\n\",t().createElement(l.ul,null,\"\\n\",t().createElement(l.li,null,t().createElement(l.code,{display:\"inline\"},'\"A, B => C\"'),\" : 왼쪽의 A, B 대신 오른쪽의 C 텀을 저장합니다. A, B 로는 C 의 검색이 가능하지만 C 로는 A, B 가 검색되지 않습니다.\"),\"\\n\",t().createElement(l.li,null,t().createElement(l.code,{display:\"inline\"},'\"A, B\"'),\" : A, B 각 텀이 A 와 B 두개의 텀을 모두 저장합니다. A 와 B 모두 서로의 검색어로 검색이 됩니다.\"),\"\\n\"),\"\\n\",t().createElement(l.p,null,\"다음은 \",t().createElement(l.strong,null,\"my_synonym\"),\" 인덱스에 \",t().createElement(l.code,{display:\"inline\"},'\"amazon => aws\"'),\" 으로 동의어를 지정하는 예제입니다.\"),\"\\n\",t().createElement(l.pre,null,t().createElement(l.code,{className:\"language-javascript\",display:\"block\"},'# \"amazon => aws\" 동의어를 지정하는 my_synonym 인덱스 생성\\nPUT my_synonym\\n{\\n  \"settings\": {\\n    \"analysis\": {\\n      \"analyzer\": {\\n        \"my_syn\": {\\n          \"tokenizer\": \"whitespace\",\\n          \"filter\": [\\n            \"lowercase\",\\n            \"syn_aws\"\\n          ]\\n        }\\n      },\\n      \"filter\": {\\n        \"syn_aws\": {\\n          \"type\": \"synonym\",\\n          \"synonyms\": [\\n            \"amazon => aws\"\\n          ]\\n        }\\n      }\\n    }\\n  },\\n  \"mappings\": {\\n    \"properties\": {\\n      \"message\": {\\n        \"type\": \"text\",\\n        \"analyzer\": \"my_syn\"\\n      }\\n    }\\n  }\\n}\\n')),\"\\n\",t().createElement(l.p,null,\"이제 여기에 \",t().createElement(l.strong,null,'\"Amazon Web Service\"'),\", \",t().createElement(l.strong,null,'\"AWS\"'),\" 값을 가진 도큐먼트 두 개를 저장하고 각 도큐먼트의 \",t().createElement(l.code,{display:\"inline\"},\"_termvectors\"),\" 를 확인 해 보겠습니다.\"),\"\\n\",t().createElement(l.pre,null,t().createElement(l.code,{className:\"language-javascript\",display:\"block\"},'# AWS, Amazon Web Service 도큐먼트 저장\\nPUT my_synonym/_doc/1\\n{ \"message\" : \"Amazon Web Service\" }\\nPUT my_synonym/_doc/2\\n{ \"message\" : \"AWS\" }\\n')),\"\\n\",t().createElement(a,null,t().createElement(s,{name:\"request\"},t().createElement(l.pre,null,t().createElement(l.code,{className:\"language-javascript\",display:\"block\"},\"# 1 도큐먼트 message 필드의 termvectors 확인\\nGET my_synonym/_termvectors/1?fields=message\\n\"))),t().createElement(s,{name:\"response\"},t().createElement(l.pre,null,t().createElement(l.code,{className:\"language-javascript\",display:\"block\"},'# 1 도큐먼트 message 필드의 termvectors 확인 결과\\n{\\n  \"_index\" : \"my_synonym\",\\n  \"_type\" : \"_doc\",\\n  \"_id\" : \"1\",\\n  \"_version\" : 1,\\n  \"found\" : true,\\n  \"took\" : 9,\\n  \"term_vectors\" : {\\n    \"message\" : {\\n      \"field_statistics\" : {\\n        \"sum_doc_freq\" : 4,\\n        \"doc_count\" : 2,\\n        \"sum_ttf\" : 4\\n      },\\n      \"terms\" : {\\n        \"aws\" : {\\n          \"term_freq\" : 1,\\n          \"tokens\" : [\\n            {\\n              \"position\" : 0,\\n              \"start_offset\" : 0,\\n              \"end_offset\" : 6\\n            }\\n          ]\\n        },\\n        \"service\" : {\\n          \"term_freq\" : 1,\\n          \"tokens\" : [\\n            {\\n              \"position\" : 2,\\n              \"start_offset\" : 11,\\n              \"end_offset\" : 18\\n            }\\n          ]\\n        },\\n        \"web\" : {\\n          \"term_freq\" : 1,\\n          \"tokens\" : [\\n            {\\n              \"position\" : 1,\\n              \"start_offset\" : 7,\\n              \"end_offset\" : 10\\n            }\\n          ]\\n        }\\n      }\\n    }\\n  }\\n}\\n')))),\"\\n\",t().createElement(a,null,t().createElement(s,{name:\"request\"},t().createElement(l.pre,null,t().createElement(l.code,{className:\"language-javascript\",display:\"block\"},\"# 2 도큐먼트 message 필드의 termvectors 확인\\nGET my_synonym/_termvectors/2?fields=message\\n\"))),t().createElement(s,{name:\"response\"},t().createElement(l.pre,null,t().createElement(l.code,{className:\"language-javascript\",display:\"block\"},'# 2 도큐먼트 message 필드의 termvectors 확인 결과\\n{\\n  \"_index\" : \"my_synonym\",\\n  \"_type\" : \"_doc\",\\n  \"_id\" : \"2\",\\n  \"_version\" : 1,\\n  \"found\" : true,\\n  \"took\" : 0,\\n  \"term_vectors\" : {\\n    \"message\" : {\\n      \"field_statistics\" : {\\n        \"sum_doc_freq\" : 4,\\n        \"doc_count\" : 2,\\n        \"sum_ttf\" : 4\\n      },\\n      \"terms\" : {\\n        \"aws\" : {\\n          \"term_freq\" : 1,\\n          \"tokens\" : [\\n            {\\n              \"position\" : 0,\\n              \"start_offset\" : 0,\\n              \"end_offset\" : 3\\n            }\\n          ]\\n        }\\n      }\\n    }\\n  }\\n}\\n')))),\"\\n\",t().createElement(l.p,null,'1 도큐먼트의 **\"Amazon Web Service\"**가 ',t().createElement(l.strong,null,\"amazon\"),\" 대신 \",t().createElement(l.strong,null,'\"aws\"'),\", \",t().createElement(l.strong,null,'\"web\"'),\", \",t().createElement(l.strong,null,'\"service\"'),\" 로 저장된 것을 확인할 수 있습니다. 다음은 각각 \",t().createElement(l.strong,null,\"term\"),\" 쿼리로 \",t().createElement(l.strong,null,\"aws\"),\", \",t().createElement(l.strong,null,\"amazon\"),\" 을 검색한 결과와 \",t().createElement(l.strong,null,\"match\"),\" 쿼리로 \",t().createElement(l.strong,null,\"amazon\"),\" 을 검색한 결과입니다.\"),\"\\n\",t().createElement(a,null,t().createElement(s,{name:\"request\"},t().createElement(l.pre,null,t().createElement(l.code,{className:\"language-javascript\",display:\"block\"},'# term 쿼리로 aws 검색\\nGET my_synonym/_search\\n{\\n  \"query\": {\\n    \"term\": {\\n      \"message\": \"aws\"\\n    }\\n  }\\n}\\n'))),t().createElement(s,{name:\"response\"},t().createElement(l.pre,null,t().createElement(l.code,{className:\"language-javascript\",display:\"block\"},'# term 쿼리로 aws 검색 결과\\n{\\n  \"took\" : 2,\\n  \"timed_out\" : false,\\n  \"_shards\" : {\\n    \"total\" : 1,\\n    \"successful\" : 1,\\n    \"skipped\" : 0,\\n    \"failed\" : 0\\n  },\\n  \"hits\" : {\\n    \"total\" : {\\n      \"value\" : 2,\\n      \"relation\" : \"eq\"\\n    },\\n    \"max_score\" : 0.22920427,\\n    \"hits\" : [\\n      {\\n        \"_index\" : \"my_synonym\",\\n        \"_type\" : \"_doc\",\\n        \"_id\" : \"2\",\\n        \"_score\" : 0.22920427,\\n        \"_source\" : {\\n          \"message\" : \"AWS\"\\n        }\\n      },\\n      {\\n        \"_index\" : \"my_synonym\",\\n        \"_type\" : \"_doc\",\\n        \"_id\" : \"1\",\\n        \"_score\" : 0.1513613,\\n        \"_source\" : {\\n          \"message\" : \"Amazon Web Service\"\\n        }\\n      }\\n    ]\\n  }\\n}\\n')))),\"\\n\",t().createElement(a,null,t().createElement(s,{name:\"request\"},t().createElement(l.pre,null,t().createElement(l.code,{className:\"language-javascript\",display:\"block\"},'# term 쿼리로 amazon 검색\\nGET my_synonym/_search\\n{\\n  \"query\": {\\n    \"term\": {\\n      \"message\": \"amazon\"\\n    }\\n  }\\n}\\n'))),t().createElement(s,{name:\"response\"},t().createElement(l.pre,null,t().createElement(l.code,{className:\"language-javascript\",display:\"block\"},'# term 쿼리로 amazon 검색 결과\\n{\\n  \"took\" : 1,\\n  \"timed_out\" : false,\\n  \"_shards\" : {\\n    \"total\" : 1,\\n    \"successful\" : 1,\\n    \"skipped\" : 0,\\n    \"failed\" : 0\\n  },\\n  \"hits\" : {\\n    \"total\" : {\\n      \"value\" : 0,\\n      \"relation\" : \"eq\"\\n    },\\n    \"max_score\" : null,\\n    \"hits\" : [ ]\\n  }\\n}\\n')))),\"\\n\",t().createElement(a,null,t().createElement(s,{name:\"request\"},t().createElement(l.pre,null,t().createElement(l.code,{className:\"language-javascript\",display:\"block\"},'# match 쿼리로 amazon 검색\\nGET my_synonym/_search\\n{\\n  \"query\": {\\n    \"match\": {\\n      \"message\": \"amazon\"\\n    }\\n  }\\n}\\n'))),t().createElement(s,{name:\"response\"},t().createElement(l.pre,null,t().createElement(l.code,{className:\"language-javascript\",display:\"block\"},'# match 쿼리로 amazon 검색 결과\\n{\\n  \"took\" : 1,\\n  \"timed_out\" : false,\\n  \"_shards\" : {\\n    \"total\" : 1,\\n    \"successful\" : 1,\\n    \"skipped\" : 0,\\n    \"failed\" : 0\\n  },\\n  \"hits\" : {\\n    \"total\" : {\\n      \"value\" : 2,\\n      \"relation\" : \"eq\"\\n    },\\n    \"max_score\" : 0.22920427,\\n    \"hits\" : [\\n      {\\n        \"_index\" : \"my_synonym\",\\n        \"_type\" : \"_doc\",\\n        \"_id\" : \"2\",\\n        \"_score\" : 0.22920427,\\n        \"_source\" : {\\n          \"message\" : \"AWS\"\\n        }\\n      },\\n      {\\n        \"_index\" : \"my_synonym\",\\n        \"_type\" : \"_doc\",\\n        \"_id\" : \"1\",\\n        \"_score\" : 0.1513613,\\n        \"_source\" : {\\n          \"message\" : \"Amazon Web Service\"\\n        }\\n      }\\n    ]\\n  }\\n}\\n')))),\"\\n\",t().createElement(l.p,null,\"첫 번째 쿼리와 마지막 세 번째 쿼리의 결과가 동일합니다.\"),\"\\n\",t().createElement(l.p,null,t().createElement(l.strong,null,\"term\"),\" 쿼리는 검색어에 애널라이저를 적용하지 않고 그대로 검색하기 때문에 \",t().createElement(l.strong,null,\"term\"),\" 쿼리로 \",t().createElement(l.strong,null,\"aws\"),\" 를 검색하면 두개의 도큐먼트가 모두 검색되고 \",t().createElement(l.strong,null,\"amazon\"),\"을 검색 하면 검색이 되지 않습니다. \",t().createElement(l.strong,null,\"match\"),\" 쿼리는 검색어 \",t().createElement(l.strong,null,\"amazon\"),\"도 \",t().createElement(l.strong,null,\"my_syn\"),\" 애널라이저가 적용이 되어 \",t().createElement(l.strong,null,\"aws\"),\" 로 변환하여 검색을 하기 때문에 \",t().createElement(l.strong,null,\"aws\"),\"로 검색을 한 것과 같은 결과가 나타납니다.\"),\"\\n\",t().createElement(l.p,null,\"이번에는 \",t().createElement(l.strong,null,\"my_synonym\"),\" 인덱스에 \",t().createElement(l.code,{display:\"inline\"},'\"amazon, aws\"'),\"  로 동의어를 지정하는 예제입니다. 기존의 \",t().createElement(l.strong,null,\"my_synonym\"),\" 인덱스를 먼저 삭제하고 입력합니다.\"),\"\\n\",t().createElement(l.pre,null,t().createElement(l.code,{className:\"language-javascript\",display:\"block\"},'# \"amazon, aws\" 동의어를 지정하는 my_synonym 인덱스 생성\\nPUT my_synonym\\n{\\n  \"settings\": {\\n    \"analysis\": {\\n      \"analyzer\": {\\n        \"my_syn\": {\\n          \"tokenizer\": \"whitespace\",\\n          \"filter\": [\\n            \"lowercase\",\\n            \"syn_aws\"\\n          ]\\n        }\\n      },\\n      \"filter\": {\\n        \"syn_aws\": {\\n          \"type\": \"synonym\",\\n          \"synonyms\": [\\n            \"amazon, aws\"\\n          ]\\n        }\\n      }\\n    }\\n  },\\n  \"mappings\": {\\n    \"properties\": {\\n      \"message\": {\\n        \"type\": \"text\",\\n        \"analyzer\": \"my_syn\"\\n      }\\n    }\\n  }\\n}\\n')),\"\\n\",t().createElement(l.p,null,\"앞의 예제와 동일하게 \",t().createElement(l.strong,null,'\"Amazon Web Service\"'),\", \",t().createElement(l.strong,null,'\"AWS\"'),\" 값을 가진 도큐먼트 두 개를 저장하고 각 도큐먼트의 \",t().createElement(l.code,{display:\"inline\"},\"_termvectors\"),\" 를 확인 해 보겠습니다.\"),\"\\n\",t().createElement(l.pre,null,t().createElement(l.code,{className:\"language-javascript\",display:\"block\"},'# AWS, Amazon Web Service 도큐먼트 저장\\nPUT my_synonym/_doc/1\\n{ \"message\" : \"Amazon Web Service\" }\\nPUT my_synonym/_doc/2\\n{ \"message\" : \"AWS\" }\\n')),\"\\n\",t().createElement(a,null,t().createElement(s,{name:\"request\"},t().createElement(l.pre,null,t().createElement(l.code,{className:\"language-javascript\",display:\"block\"},\"# 1 도큐먼트 message 필드의 termvectors 확인\\nGET my_synonym/_termvectors/1?fields=message\\n\"))),t().createElement(s,{name:\"response\"},t().createElement(l.pre,null,t().createElement(l.code,{className:\"language-javascript\",display:\"block\"},'# 1 도큐먼트 message 필드의 termvectors 확인 결과\\n{\\n  \"_index\" : \"my_synonym\",\\n  \"_type\" : \"_doc\",\\n  \"_id\" : \"1\",\\n  \"_version\" : 1,\\n  \"found\" : true,\\n  \"took\" : 0,\\n  \"term_vectors\" : {\\n    \"message\" : {\\n      \"field_statistics\" : {\\n        \"sum_doc_freq\" : 6,\\n        \"doc_count\" : 2,\\n        \"sum_ttf\" : 6\\n      },\\n      \"terms\" : {\\n        \"amazon\" : {\\n          \"term_freq\" : 1,\\n          \"tokens\" : [\\n            {\\n              \"position\" : 0,\\n              \"start_offset\" : 0,\\n              \"end_offset\" : 6\\n            }\\n          ]\\n        },\\n        \"aws\" : {\\n          \"term_freq\" : 1,\\n          \"tokens\" : [\\n            {\\n              \"position\" : 0,\\n              \"start_offset\" : 0,\\n              \"end_offset\" : 6\\n            }\\n          ]\\n        },\\n        \"service\" : {\\n          \"term_freq\" : 1,\\n          \"tokens\" : [\\n            {\\n              \"position\" : 2,\\n              \"start_offset\" : 11,\\n              \"end_offset\" : 18\\n            }\\n          ]\\n        },\\n        \"web\" : {\\n          \"term_freq\" : 1,\\n          \"tokens\" : [\\n            {\\n              \"position\" : 1,\\n              \"start_offset\" : 7,\\n              \"end_offset\" : 10\\n            }\\n          ]\\n        }\\n      }\\n    }\\n  }\\n}\\n')))),\"\\n\",t().createElement(a,null,t().createElement(s,{name:\"request\"},t().createElement(l.pre,null,t().createElement(l.code,{className:\"language-javascript\",display:\"block\"},\"# 2 도큐먼트 message 필드의 termvectors 확인\\nGET my_synonym/_termvectors/2?fields=message\\n\"))),t().createElement(s,{name:\"response\"},t().createElement(l.pre,null,t().createElement(l.code,{className:\"language-javascript\",display:\"block\"},'# 2 도큐먼트 message 필드의 termvectors 확인 결과\\n{\\n  \"_index\" : \"my_synonym\",\\n  \"_type\" : \"_doc\",\\n  \"_id\" : \"2\",\\n  \"_version\" : 1,\\n  \"found\" : true,\\n  \"took\" : 0,\\n  \"term_vectors\" : {\\n    \"message\" : {\\n      \"field_statistics\" : {\\n        \"sum_doc_freq\" : 6,\\n        \"doc_count\" : 2,\\n        \"sum_ttf\" : 6\\n      },\\n      \"terms\" : {\\n        \"amazon\" : {\\n          \"term_freq\" : 1,\\n          \"tokens\" : [\\n            {\\n              \"position\" : 0,\\n              \"start_offset\" : 0,\\n              \"end_offset\" : 3\\n            }\\n          ]\\n        },\\n        \"aws\" : {\\n          \"term_freq\" : 1,\\n          \"tokens\" : [\\n            {\\n              \"position\" : 0,\\n              \"start_offset\" : 0,\\n              \"end_offset\" : 3\\n            }\\n          ]\\n        }\\n      }\\n    }\\n  }\\n}\\n')))),\"\\n\",t().createElement(l.p,null,\"두 도큐먼트 모두 \",t().createElement(l.code,{display:\"inline\"},'\"position\" : 0'),\" 위치에 \",t().createElement(l.strong,null,'\"aws\"'),\", \",t().createElement(l.strong,null,'\"amazon\"'),\" 두개의 텀들이 모두 저장된 것을 확인할 수 있습니다. 이제 \",t().createElement(l.strong,null,\"term\"),\" 쿼리로 \",t().createElement(l.strong,null,\"amazon\"),\" 을 검색해도 두개 도큐먼트가 모두 검색이 됩니다. 이것은 한번 직접 실행 해 보시기 바랍니다.\"),\"\\n\",t().createElement(l.p,null,\"동의어 여러 개를 입력 할 때는 \",t().createElement(l.code,{display:\"inline\"},'\"synonyms\": [ ... ]'),\" 항목 안에 배열로 넣어도 되지만, 그 보다는 파일을 따로 만들어 관리하는 것이 편합니다. \",t().createElement(l.strong,null,\"stop\"),\" 토큰 필터와 마찬가지로 \",t().createElement(l.strong,null,\"synonyms_path\"),\" 항목에 \",t().createElement(l.strong,null,\"config\"),\" 디렉토리 기준의 상대 경로에 파일을 저장하고 경로명을 입력하면 됩니다. 동의어는 하나의 규칙당 \",t().createElement(l.strong,null,\"한 줄씩\"),\" 입력해야 하며 파일은 \",t().createElement(l.strong,null,\"UTF-8\"),\"로 인코딩 되어야 합니다.\"),\"\\n\",t().createElement(l.p,null,\"다음은 \",t().createElement(l.strong,null,'\"hop, jump\"'),\", \",t().createElement(l.strong,null,'\"quick, fast\"'),\" 를 \",t().createElement(l.strong,null,\"user_dic/my_syn_dic.txt\"),\" 에 저장해서 동의어 사전으로 사용하는 예제입니다.\"),\"\\n\",t().createElement(l.pre,null,t().createElement(l.code,{className:\"language-bash\",display:\"block\"},\"# config/user_dic 디렉토리 아래에 my_syn_dic.txt 파일 생성\\n$ echo 'quick, fast\\nhop, jump' > config/user_dic/my_syn_dic.txt\\n\")),\"\\n\",t().createElement(l.pre,null,t().createElement(l.code,{className:\"language-javascript\",display:\"block\"},'# synonyms_path 설정을 가진 my_synonym 인덱스 생성\\nPUT my_synonym\\n{\\n  \"settings\": {\\n    \"analysis\": {\\n      \"analyzer\": {\\n        \"my_syn\": {\\n          \"tokenizer\": \"whitespace\",\\n          \"filter\": [\\n            \"lowercase\",\\n            \"syn_aws\"\\n          ]\\n        }\\n      },\\n      \"filter\": {\\n        \"syn_aws\": {\\n          \"type\": \"synonym\",\\n          \"synonyms_path\": \"user_dic/my_syn_dic.txt\"\\n        }\\n      }\\n    }\\n  },\\n  \"mappings\": {\\n    \"properties\": {\\n      \"message\": {\\n        \"type\": \"text\",\\n        \"analyzer\": \"my_syn\"\\n      }\\n    }\\n  }\\n}\\n')),\"\\n\",t().createElement(l.p,null,\"이제 \",t().createElement(l.strong,null,\"term\"),\" 쿼리로 \",t().createElement(l.strong,null,\"quick\"),\" 과 \",t().createElement(l.strong,null,\"jump\"),\"를 검색하면 \",t().createElement(l.strong,null,\"hop\"),\", \",t().createElement(l.strong,null,\"fast\"),\" 도 검색이 됩니다.\"),\"\\n\",t().createElement(l.pre,null,t().createElement(l.code,{className:\"language-javascript\",display:\"block\"},'# quick, jump, hop, fast 를 포함하는 도큐먼트 저장\\nPUT my_synonym/_doc/1\\n{ \"message\": \"Quick brown fox jump\" }\\nPUT my_synonym/_doc/2\\n{ \"message\": \"hop rabbit is fast\" }\\n')),\"\\n\",t().createElement(a,null,t().createElement(s,{name:\"request\"},t().createElement(l.pre,null,t().createElement(l.code,{className:\"language-javascript\",display:\"block\"},'# term 쿼리로 quick, jump 검색\\nGET my_synonym/_search\\n{\\n  \"query\": {\\n    \"bool\": {\\n      \"must\": [\\n        {\\n          \"term\": {\\n            \"message\": \"quick\"\\n          }\\n        },\\n        {\\n          \"term\": {\\n            \"message\": \"jump\"\\n          }\\n        }\\n      ]\\n    }\\n  }\\n}\\n'))),t().createElement(s,{name:\"response\"},t().createElement(l.pre,null,t().createElement(l.code,{className:\"language-javascript\",display:\"block\"},'# term 쿼리로 quick, jump 검색 결과\\n{\\n  \"took\" : 370,\\n  \"timed_out\" : false,\\n  \"_shards\" : {\\n    \"total\" : 1,\\n    \"successful\" : 1,\\n    \"skipped\" : 0,\\n    \"failed\" : 0\\n  },\\n  \"hits\" : {\\n    \"total\" : {\\n      \"value\" : 2,\\n      \"relation\" : \"eq\"\\n    },\\n    \"max_score\" : 0.42221838,\\n    \"hits\" : [\\n      {\\n        \"_index\" : \"my_synonym\",\\n        \"_type\" : \"_doc\",\\n        \"_id\" : \"1\",\\n        \"_score\" : 0.42221838,\\n        \"_source\" : {\\n          \"message\" : \"Quick brown fox jump\"\\n        }\\n      },\\n      {\\n        \"_index\" : \"my_synonym\",\\n        \"_type\" : \"_doc\",\\n        \"_id\" : \"2\",\\n        \"_score\" : 0.42221838,\\n        \"_source\" : {\\n          \"message\" : \"hop rabbit is fast\"\\n        }\\n      }\\n    ]\\n  }\\n}\\n')))),\"\\n\",t().createElement(l.p,null,\"마지막으로 \",t().createElement(l.strong,null,\"synonym\"),\" 토큰 필터에는 추가적으로 다음과 같은 옵션들이 있습니다.\"),\"\\n\",t().createElement(l.ul,null,\"\\n\",t().createElement(l.li,null,t().createElement(l.strong,null,\"expand\"),\" (true / false. 디폴트는 \",t().createElement(l.strong,null,\"true\"),\")\\n\",t().createElement(l.code,{display:\"inline\"},'\"expand\": false'),\" 로 설정하게 되면 \",t().createElement(l.code,{display:\"inline\"},'\"synonyms\": \"aws, amazon\"'),\" 같은 설정에 토큰들을 모두 저장하지 않고 맨 처음에 명시된 토큰 하나만 저장합니다. 앞의 설정은 \",t().createElement(l.code,{display:\"inline\"},'\"synonyms\": \"aws, amazon => aws\"'),\" 로 설정한 것과 동일하게 동작합니다.\"),\"\\n\",t().createElement(l.li,null,t().createElement(l.strong,null,\"lenient\"),\" (true / false. 디폴트는 \",t().createElement(l.strong,null,\"false\"),\")\\n\",t().createElement(l.code,{display:\"inline\"},'\"lenient\": true'),\" 로 설정하면 synonym 설정에 오류가 있는 경우 오류가 있는 부분을 무시하고 실행합니다.\"),\"\\n\"),\"\\n\",t().createElement(l.h2,{id:\"664-ngram-edge-ngram-shingle\"},\"6.6.4 NGram, Edge NGram, Shingle\"),\"\\n\",t().createElement(l.h3,{id:\"ngram\"},\"NGram\"),\"\\n\",t().createElement(l.p,null,\"Elasticsearch는 빠른 검색을 위해 검색에 사용될 텀 들을 미리 분리해서 역 인덱스에 저장합니다. 하지만 과학 용어집 검색 같은 특정한 사용 사례에 따라 텀이 아닌 단어의 일부만 가지고도 검색해야 하는 기능이 필요한 경우도 있습니다. RDBMS의 LIKE 검색 처럼 사용하는 \",t().createElement(l.strong,null,\"wildcard\"),\" 쿼리나 \",t().createElement(l.strong,null,\"regexp (정규식)\"),\" 쿼리도 지원을 하지만, 이런 쿼리들은 메모리 소모가 많고 느리기 때문에 Elasticsearch의 장점을 활용하지 못합니다. 이런 사용을 위해 검색 텀의 일부만 미리 분리해서 저장을 할 수 있는데 이렇게 단어의 일부를 나눈 부위를 \",t().createElement(l.strong,null,\"NGram\"),\" 이라고 합니다. 보통은 \",t().createElement(l.strong,null,\"unigram\"),\"(유니그램 – 1글자), \",t().createElement(l.strong,null,\"bigram\"),\"(바이그램 - 2자) 등으로 부릅니다.\"),\"\\n\",t().createElement(l.p,null,\"Elasticsearch는 \",t().createElement(l.strong,null,\"NGram\"),\"을 처리하는 토큰 필터를 제공하며 설정은 \",t().createElement(l.code,{display:\"inline\"},'\"type\": \"nGram\"'),\" 으로 합니다. \",t().createElement(l.strong,null,'\"house\"'),' 라는 단어를 2 글자의 NGram (bigram) 으로 처리하면 다음과 같이 \"',t().createElement(l.strong,null,'ho\"'),\", \",t().createElement(l.strong,null,'\"ou\"'),\", \",t().createElement(l.strong,null,'\"us\"'),\", \",t().createElement(l.strong,null,'\"se\"'),\" 총 4개의 토큰들이 추출됩니다. ngram 토큰필터를 사용하면 이렇게 2글자씩 추출된 텀들이 모두 검색 토큰으로 저장됩니다. 이제 이 인덱스의 경우에는 검색어를 \",t().createElement(l.strong,null,'\"ho\"'),\" 라고만 검색을 해도 \",t().createElement(l.strong,null,\"ho\"),\"use 가 포함된 도큐먼트들이 검색이 됩니다.\"),\"\\n\",t().createElement(l.p,null,t().createElement(l.img,{src:\"/ZWxhc3RpYy9kb2NzLWtyL2VzZ3VpZGU3/assets/06-06_ngram_house.png\",alt:\"단어 house를 ngram(2) 으로 분리\"})),\"\\n\",t().createElement(o,{color:\"danger\"},t().createElement(l.p,null,t().createElement(l.strong,null,\"ngram\"),\" 토큰필터를 사용하면 저장되는 텀의 갯수도 기하급수적으로 늘어나고 검색어를 \",t().createElement(l.strong,null,'\"ho\"',t().createElement(l.strong,null,\"로 검색 했을 때 \",t().createElement(l.strong,null,\"ho\"),\"use, s\"),\"ho\"),\"es 처럼 검색 결과를 예상하기 어렵기 때문에 일반적인 텍스트 검색에는 사용하지 않는 것이 좋습니다. ngram을 사용하기 적합한 사례는 카테고리 목록이나 태그 목록과 같이 전체 개수가 많지 않은 데이터 집단에 \",t().createElement(l.strong,null,\"자동완성\"),\" 같은 기능을 구현하는 데에 적합합니다.\")),\"\\n\",t().createElement(l.p,null,\"ngram 토큰 필터에는 \",t().createElement(l.code,{display:\"inline\"},\"min_gram\"),\" (디폴트 1), \",t().createElement(l.code,{display:\"inline\"},\"max_gram\"),\" (디폴트 2) 옵션이 있습니다. 짐작할 수 있듯이 최소, 최대 문자수의 토큰을 구분하는 단위입니다. \",t().createElement(l.strong,null,\"house\"),\"를 \",t().createElement(l.code,{display:\"inline\"},'\"min_gram\": 2'),\", \",t().createElement(l.code,{display:\"inline\"},'\"max_gram\": 3'),\" 으로 설정하면 다음과 같이 분석되어 총 7개의 토큰을 저장합니다.\"),\"\\n\",t().createElement(l.p,null,t().createElement(l.img,{src:\"/ZWxhc3RpYy9kb2NzLWtyL2VzZ3VpZGU3/assets/06-06_ngram_house_2.png\",alt:\"단어 house를 ngram(2-3) 으로 분리\"})),\"\\n\",t().createElement(l.p,null,\"다음은 \",t().createElement(l.strong,null,\"my_ngram\"),\" 인덱스에 \",t().createElement(l.code,{display:\"inline\"},'\"min_gram\": 2'),\", \",t().createElement(l.code,{display:\"inline\"},'\"max_gram\": 3'),\" 인 \",t().createElement(l.strong,null,\"my_ngram_f\"),\" 토큰필터를 만들고 house 를 분석하는 예제입니다.\"),\"\\n\",t().createElement(l.pre,null,t().createElement(l.code,{className:\"language-javascript\",display:\"block\"},'PUT my_ngram\\n{\\n  \"settings\": {\\n    \"analysis\": {\\n      \"filter\": {\\n        \"my_ngram_f\": {\\n          \"type\": \"nGram\",\\n          \"min_gram\": 2,\\n          \"max_gram\": 3\\n        }\\n      }\\n    }\\n  }\\n}\\n')),\"\\n\",t().createElement(a,null,t().createElement(s,{name:\"request\"},t().createElement(l.pre,null,t().createElement(l.code,{className:\"language-javascript\",display:\"block\"},'# my_ngram_f 토큰필터로 \"house\" 분석\\nGET my_ngram/_analyze\\n{\\n  \"tokenizer\": \"keyword\",\\n  \"filter\": [\\n    \"my_ngram_f\"\\n  ],\\n  \"text\": \"house\"\\n}\\n'))),t().createElement(s,{name:\"response\"},t().createElement(l.pre,null,t().createElement(l.code,{className:\"language-javascript\",display:\"block\"},'# my_ngram_f 토큰필터로 \"house\" 분석 결과\\n{\\n  \"tokens\" : [\\n    {\\n      \"token\" : \"ho\",\\n      \"start_offset\" : 0,\\n      \"end_offset\" : 5,\\n      \"type\" : \"word\",\\n      \"position\" : 0\\n    },\\n    {\\n      \"token\" : \"hou\",\\n      \"start_offset\" : 0,\\n      \"end_offset\" : 5,\\n      \"type\" : \"word\",\\n      \"position\" : 0\\n    },\\n    {\\n      \"token\" : \"ou\",\\n      \"start_offset\" : 0,\\n      \"end_offset\" : 5,\\n      \"type\" : \"word\",\\n      \"position\" : 0\\n    },\\n    {\\n      \"token\" : \"ous\",\\n      \"start_offset\" : 0,\\n      \"end_offset\" : 5,\\n      \"type\" : \"word\",\\n      \"position\" : 0\\n    },\\n    {\\n      \"token\" : \"us\",\\n      \"start_offset\" : 0,\\n      \"end_offset\" : 5,\\n      \"type\" : \"word\",\\n      \"position\" : 0\\n    },\\n    {\\n      \"token\" : \"use\",\\n      \"start_offset\" : 0,\\n      \"end_offset\" : 5,\\n      \"type\" : \"word\",\\n      \"position\" : 0\\n    },\\n    {\\n      \"token\" : \"se\",\\n      \"start_offset\" : 0,\\n      \"end_offset\" : 5,\\n      \"type\" : \"word\",\\n      \"position\" : 0\\n    }\\n  ]\\n}\\n')))),\"\\n\",t().createElement(l.h3,{id:\"edge-ngram\"},\"Edge NGram\"),\"\\n\",t().createElement(l.p,null,\"검색을 위해 NGram을 저장하더라도 보통은 단어의 맨 앞에서부터 검색하는 경우가 많습니다. 텀 앞쪽의 ngram 만 저장하기 위해서는 \",t().createElement(l.strong,null,\"Edge NGram\"),\" 토큰필터를 이용합니다. 설정 방법은 \",t().createElement(l.code,{display:\"inline\"},'\"type\": \"edgeNGram\"'),\" 입니다. edgeNGram의 옵션을 \",t().createElement(l.code,{display:\"inline\"},'\"min_gram\": 1'),\", \",t().createElement(l.code,{display:\"inline\"},'\"max_gram\": 4'),' 으로 설정하고 \"',t().createElement(l.strong,null,'house\"'),\" 를 분석하면 다음과 같이 \",t().createElement(l.strong,null,'\"h\", \"ho\", \"hou\", \"hous\"'),\" 4개의 토큰이 생성됩니다.\"),\"\\n\",t().createElement(l.p,null,t().createElement(l.img,{src:\"/ZWxhc3RpYy9kb2NzLWtyL2VzZ3VpZGU3/assets/06-06_edngram.png\",alt:\"단어 house를 edgeNgram 으로 분리\"})),\"\\n\",t().createElement(l.p,null,\"인덱스 설정과 쿼리를 이용한 애널라이저 분석은 위의 NGram 예제를 참고해서 직접 만들어 보시기 바랍니다.\"),\"\\n\",t().createElement(l.h3,{id:\"shingle\"},\"Shingle\"),\"\\n\",t().createElement(l.p,null,t().createElement(l.strong,null,\"NGram\"),\"과 \",t().createElement(l.strong,null,\"Edge NGram\"),\"은 모두 하나의 단어로부터 토큰을 확장하는 토큰 필터입니다. 문자가 아니라 단어 단위로 구성된 묶음을 \",t().createElement(l.strong,null,\"Shingle\"),\" 이라고 하며 \",t().createElement(l.code,{display:\"inline\"},'\"type\": \"shingle\"'),\" 토큰 필터의 이용이 가능합니다. \",t().createElement(l.strong,null,'\"this is my sweet home\"'),\" 라는 문장을 분리해서 \",t().createElement(l.strong,null,\"2 단어씩 Shingle\"),\" 토큰 필터를 적용하면 다음과 같은 4개의 \",t().createElement(l.strong,null,\"shingle\"),\" 들이 생성됩니다.\"),\"\\n\",t().createElement(l.p,null,t().createElement(l.img,{src:\"/ZWxhc3RpYy9kb2NzLWtyL2VzZ3VpZGU3/assets/06-06_shingle.png\",alt:\"this is my sweet home 문장을 shingle 로 분리\"})),\"\\n\",t().createElement(l.p,null,t().createElement(l.strong,null,\"Shingle\"),\" 토큰 필터에서 사용 가능한 옵션은 다음과 같습니다.\"),\"\\n\",t().createElement(l.ul,null,\"\\n\",t().createElement(l.li,null,t().createElement(l.strong,null,\"min_shingle_size\"),\" / \",t().createElement(l.strong,null,\"max_shingle_size\"),\" : shingle의 최소 / 최대 단어 개수를 지정합니다. 디폴트는 모두 2 입니다.\"),\"\\n\",t().createElement(l.li,null,t().createElement(l.strong,null,\"output_unigrams\"),\" : Shingle 외에도 각각의 개별 토큰(unigram)도 저장 하는지의 여부를 설정합니다. 디폴트는 true 입니다.\"),\"\\n\",t().createElement(l.li,null,t().createElement(l.strong,null,\"output_unigrams_if_no_shingles\"),\" : shingle 을 만들 수 없는 경우에만 개별 토큰을 저장하는지의 여부를 설정합니다. 디폴트는 false 입니다.\"),\"\\n\",t().createElement(l.li,null,t().createElement(l.strong,null,\"token_separator\"),\" : 토큰 구분자를 지정합니다. 디폴트는 \",t().createElement(l.code,{display:\"inline\"},'\" \"'),\" (스페이스) 입니다.\"),\"\\n\",t().createElement(l.li,null,t().createElement(l.strong,null,\"filler_token\"),\" : shing을 만들 텀이 없는 경우 (보통은 stop 토큰 필터와 함께 사용되어 offset 위치만 있고 텀이 없는 경우입니다) 대체할 텍스트를 지정합니다. 디폴트는 \",t().createElement(l.code,{display:\"inline\"},\"_\"),\" 입니다.\"),\"\\n\"),\"\\n\",t().createElement(l.p,null,\"다음은 \",t().createElement(l.strong,null,\"my_shingle\"),\" 인덱스에서 \",t().createElement(l.code,{display:\"inline\"},'\"min_shingle_size\": 3'),\", \",t().createElement(l.code,{display:\"inline\"},'\"max_shingle_size\": 4'),\" 로 설정해서 \",t().createElement(l.strong,null,'\"this is my sweet home\"'),\" 문장을 분석하는 예제입니다.\"),\"\\n\",t().createElement(l.pre,null,t().createElement(l.code,{className:\"language-javascript\",display:\"block\"},'PUT my_shingle\\n{\\n  \"settings\": {\\n    \"analysis\": {\\n      \"filter\": {\\n        \"my_shingle_f\": {\\n          \"type\": \"shingle\",\\n          \"min_shingle_size\": 3,\\n          \"max_shingle_size\": 4\\n        }\\n      }\\n    }\\n  }\\n}\\n')),\"\\n\",t().createElement(a,null,t().createElement(s,{name:\"request\"},t().createElement(l.pre,null,t().createElement(l.code,{className:\"language-javascript\",display:\"block\"},'# my_shingle_f 토큰필터로 \"this is my sweet home\" 분석\\nGET my_shingle/_analyze\\n{\\n  \"tokenizer\": \"whitespace\",\\n  \"filter\": [\\n    \"my_shingle_f\"\\n  ],\\n  \"text\": \"this is my sweet home\"\\n}\\n'))),t().createElement(s,{name:\"response\"},t().createElement(l.pre,null,t().createElement(l.code,{className:\"language-javascript\",display:\"block\"},'# my_shingle_f 토큰필터로 \"this is my sweet home\" 분석 결과\\n{\\n  \"tokens\" : [\\n    {\\n      \"token\" : \"this\",\\n      \"start_offset\" : 0,\\n      \"end_offset\" : 4,\\n      \"type\" : \"word\",\\n      \"position\" : 0\\n    },\\n    {\\n      \"token\" : \"this is my\",\\n      \"start_offset\" : 0,\\n      \"end_offset\" : 10,\\n      \"type\" : \"shingle\",\\n      \"position\" : 0,\\n      \"positionLength\" : 3\\n    },\\n    {\\n      \"token\" : \"this is my sweet\",\\n      \"start_offset\" : 0,\\n      \"end_offset\" : 16,\\n      \"type\" : \"shingle\",\\n      \"position\" : 0,\\n      \"positionLength\" : 4\\n    },\\n    {\\n      \"token\" : \"is\",\\n      \"start_offset\" : 5,\\n      \"end_offset\" : 7,\\n      \"type\" : \"word\",\\n      \"position\" : 1\\n    },\\n    {\\n      \"token\" : \"is my sweet\",\\n      \"start_offset\" : 5,\\n      \"end_offset\" : 16,\\n      \"type\" : \"shingle\",\\n      \"position\" : 1,\\n      \"positionLength\" : 3\\n    },\\n    {\\n      \"token\" : \"is my sweet home\",\\n      \"start_offset\" : 5,\\n      \"end_offset\" : 21,\\n      \"type\" : \"shingle\",\\n      \"position\" : 1,\\n      \"positionLength\" : 4\\n    },\\n    {\\n      \"token\" : \"my\",\\n      \"start_offset\" : 8,\\n      \"end_offset\" : 10,\\n      \"type\" : \"word\",\\n      \"position\" : 2\\n    },\\n    {\\n      \"token\" : \"my sweet home\",\\n      \"start_offset\" : 8,\\n      \"end_offset\" : 21,\\n      \"type\" : \"shingle\",\\n      \"position\" : 2,\\n      \"positionLength\" : 3\\n    },\\n    {\\n      \"token\" : \"sweet\",\\n      \"start_offset\" : 11,\\n      \"end_offset\" : 16,\\n      \"type\" : \"word\",\\n      \"position\" : 3\\n    },\\n    {\\n      \"token\" : \"home\",\\n      \"start_offset\" : 17,\\n      \"end_offset\" : 21,\\n      \"type\" : \"word\",\\n      \"position\" : 4\\n    }\\n  ]\\n}\\n')))),\"\\n\",t().createElement(l.p,null,t().createElement(l.code,{display:\"inline\"},'\"token\" : \"this is my\"'),\", \",t().createElement(l.code,{display:\"inline\"},'\"token\" : \"this is my sweet\"'),\" 와 같은 토큰들이 저장된 것을 확인할 수 있습니다. 이번에는 \",t().createElement(l.code,{display:\"inline\"},'\"output_unigrams\": false'),\" 와 \",t().createElement(l.code,{display:\"inline\"},'\"filler_token\": \"-\"'),\" 설정을 추가하고 stop 토큰필터로 \",t().createElement(l.code,{display:\"inline\"},'\"is\"'),\" 를 불용어 처리한 뒤 실행 해 보겠습니다.\"),\"\\n\",t().createElement(l.pre,null,t().createElement(l.code,{className:\"language-javascript\",display:\"block\"},'PUT my_shingle\\n{\\n  \"settings\": {\\n    \"analysis\": {\\n      \"filter\": {\\n        \"my_shingle_f\": {\\n          \"type\": \"shingle\",\\n          \"min_shingle_size\": 3,\\n          \"max_shingle_size\": 4,\\n          \"output_unigrams\": false,\\n          \"filler_token\": \"-\"\\n        },\\n        \"my_stop_f\": {\\n          \"type\": \"stop\",\\n          \"stopwords\": [\\n            \"is\"\\n          ]\\n        }\\n      }\\n    }\\n  }\\n}\\n')),\"\\n\",t().createElement(a,null,t().createElement(s,{name:\"request\"},t().createElement(l.pre,null,t().createElement(l.code,{className:\"language-javascript\",display:\"block\"},'# my_shingle 에서 \"this is my sweet home\" 분석\\nGET my_shingle/_analyze\\n{\\n  \"tokenizer\": \"whitespace\",\\n  \"filter\": [\\n    \"my_stop_f\",\\n    \"my_shingle_f\"\\n  ],\\n  \"text\": \"this is my sweet home\"\\n}\\n'))),t().createElement(s,{name:\"response\"},t().createElement(l.pre,null,t().createElement(l.code,{className:\"language-javascript\",display:\"block\"},'# my_shingle 에서 \"this is my sweet home\" 분석 결과\\n{\\n  \"tokens\" : [\\n    {\\n      \"token\" : \"this - my\",\\n      \"start_offset\" : 0,\\n      \"end_offset\" : 10,\\n      \"type\" : \"shingle\",\\n      \"position\" : 0\\n    },\\n    {\\n      \"token\" : \"this - my sweet\",\\n      \"start_offset\" : 0,\\n      \"end_offset\" : 16,\\n      \"type\" : \"shingle\",\\n      \"position\" : 0,\\n      \"positionLength\" : 2\\n    },\\n    {\\n      \"token\" : \"- my sweet\",\\n      \"start_offset\" : 8,\\n      \"end_offset\" : 16,\\n      \"type\" : \"shingle\",\\n      \"position\" : 1\\n    },\\n    {\\n      \"token\" : \"- my sweet home\",\\n      \"start_offset\" : 8,\\n      \"end_offset\" : 21,\\n      \"type\" : \"shingle\",\\n      \"position\" : 1,\\n      \"positionLength\" : 2\\n    },\\n    {\\n      \"token\" : \"my sweet home\",\\n      \"start_offset\" : 8,\\n      \"end_offset\" : 21,\\n      \"type\" : \"shingle\",\\n      \"position\" : 2\\n    }\\n  ]\\n}\\n')))),\"\\n\",t().createElement(l.p,null,\"단일 토큰들은 모두 삭제되었고 \",t().createElement(l.code,{display:\"inline\"},'\"is\"'),\" 는 \",t().createElement(l.code,{display:\"inline\"},'\"-\"'),\" 로 대치된 3, 4개 단어로 이루어진 shingle 들이 생성된 것을 확인할 수 있습니다.\"),\"\\n\",t().createElement(o,{color:\"warning\"},t().createElement(l.p,null,t().createElement(l.strong,null,\"NGram\"),\", \",t().createElement(l.strong,null,\"Edged NGram\"),\" 그리고 \",t().createElement(l.strong,null,\"Shingle\"),\" 토큰 필터는 보통 일반적인 텍스트 분석에 사용하기는 적합하지 않습니다. 하지만 자동 완성 기능을 구현하거나 프로그램 코드 안에서 문법이나 기능명을 검색하는 것과 같이 특수한 요구사항을 충족해야 하는 경우 유용하게 사용될 수 있습니다.\")),\"\\n\",t().createElement(l.h2,{id:\"665-unique\"},\"6.6.5 Unique\"),\"\\n\",t().createElement(l.p,null,t().createElement(l.strong,null,'\"white fox, white rabbit, white bear\"'),\" 같은 문장을 분석하면 \",t().createElement(l.strong,null,'\"white\"'),\" 텀은 총 3번 저장이 됩니다. 역 색인에는 텀이 1개만 있어도 텀을 포함하는 도큐먼트를 가져올 수 있기 때문에 중복되는 텀 들은 삭제 해 주어도 검색에는 보통 무방합니다. 이 경우 \",t().createElement(l.strong,null,\"unique\"),\" 토큰 필터를 사용해서 중복되는 텀 들은 하나만 저장하도록 할 수 있습니다.\"),\"\\n\",t().createElement(l.p,null,\"다음은 \",t().createElement(l.strong,null,'\"white fox, white rabbit, white bear\"'),\" 문장을 \",t().createElement(l.strong,null,\"unique\"),\" 토큰 필터를 적용하지 않은 경우와 적용한 경우를 비교 한 예제입니다.\"),\"\\n\",t().createElement(a,null,t().createElement(s,{name:\"request\"},t().createElement(l.pre,null,t().createElement(l.code,{className:\"language-javascript\",display:\"block\"},'# 일반적인 \"white fox, white rabbit, white bear\" 문장 분석\\nGET _analyze\\n{\\n  \"tokenizer\": \"standard\",\\n  \"filter\": [\\n    \"lowercase\"\\n  ],\\n  \"text\": [\\n    \"white fox, white rabbit, white bear\"\\n  ]\\n}\\n'))),t().createElement(s,{name:\"response\"},t().createElement(l.pre,null,t().createElement(l.code,{className:\"language-javascript\",display:\"block\"},'# 일반적인 \"white fox, white rabbit, white bear\" 문장 분석 결과\\n{\\n  \"tokens\" : [\\n    {\\n      \"token\" : \"white\",\\n      \"start_offset\" : 0,\\n      \"end_offset\" : 5,\\n      \"type\" : \"<ALPHANUM>\",\\n      \"position\" : 0\\n    },\\n    {\\n      \"token\" : \"fox\",\\n      \"start_offset\" : 6,\\n      \"end_offset\" : 9,\\n      \"type\" : \"<ALPHANUM>\",\\n      \"position\" : 1\\n    },\\n    {\\n      \"token\" : \"white\",\\n      \"start_offset\" : 11,\\n      \"end_offset\" : 16,\\n      \"type\" : \"<ALPHANUM>\",\\n      \"position\" : 2\\n    },\\n    {\\n      \"token\" : \"rabbit\",\\n      \"start_offset\" : 17,\\n      \"end_offset\" : 23,\\n      \"type\" : \"<ALPHANUM>\",\\n      \"position\" : 3\\n    },\\n    {\\n      \"token\" : \"white\",\\n      \"start_offset\" : 25,\\n      \"end_offset\" : 30,\\n      \"type\" : \"<ALPHANUM>\",\\n      \"position\" : 4\\n    },\\n    {\\n      \"token\" : \"bear\",\\n      \"start_offset\" : 31,\\n      \"end_offset\" : 35,\\n      \"type\" : \"<ALPHANUM>\",\\n      \"position\" : 5\\n    }\\n  ]\\n}\\n')))),\"\\n\",t().createElement(a,null,t().createElement(s,{name:\"request\"},t().createElement(l.pre,null,t().createElement(l.code,{className:\"language-javascript\",display:\"block\"},'# unique 토크나이저로 \"white fox, white rabbit, white bear\" 문장 분석\\nGET _analyze\\n{\\n  \"tokenizer\": \"standard\",\\n  \"filter\": [\\n    \"lowercase\",\\n    \"unique\"\\n  ],\\n  \"text\": [\\n    \"white fox, white rabbit, white bear\"\\n  ]\\n}\\n'))),t().createElement(s,{name:\"response\"},t().createElement(l.pre,null,t().createElement(l.code,{className:\"language-javascript\",display:\"block\"},'# unique 토크나이저로 \"white fox, white rabbit, white bear\" 문장 분석 결과\\n{\\n  \"tokens\" : [\\n    {\\n      \"token\" : \"white\",\\n      \"start_offset\" : 0,\\n      \"end_offset\" : 5,\\n      \"type\" : \"<ALPHANUM>\",\\n      \"position\" : 0\\n    },\\n    {\\n      \"token\" : \"fox\",\\n      \"start_offset\" : 6,\\n      \"end_offset\" : 9,\\n      \"type\" : \"<ALPHANUM>\",\\n      \"position\" : 1\\n    },\\n    {\\n      \"token\" : \"rabbit\",\\n      \"start_offset\" : 17,\\n      \"end_offset\" : 23,\\n      \"type\" : \"<ALPHANUM>\",\\n      \"position\" : 2\\n    },\\n    {\\n      \"token\" : \"bear\",\\n      \"start_offset\" : 31,\\n      \"end_offset\" : 35,\\n      \"type\" : \"<ALPHANUM>\",\\n      \"position\" : 3\\n    }\\n  ]\\n}\\n')))),\"\\n\",t().createElement(l.p,null,t().createElement(l.code,{display:\"inline\"},'\"unique\"'),\" 토큰 필터를 적용하면 \",t().createElement(l.strong,null,'\"white\"'),\" 텀이 하나만 저장된 것을 확인할 수 있습니다.\"),\"\\n\",t().createElement(o,{color:\"danger\"},t().createElement(l.p,null,\"match 쿼리를 사용해서 검색하는 경우 unique 토큰 필터를 적용한 필드는 텀의 개수가 1개로 되기 때문에 \",t().createElement(l.strong,null,\"TF(Term Frequency)\"),\" 값이 줄어들어 스코어 점수가 달라질 수 있습니다. match 쿼리를 이용해 \",t().createElement(l.strong,null,\"정확도(relevancy)\"),\" 를 따져야 하는 검색의 경우에는 unique 토큰 필터는 사용하지 않는 것이 바람직합니다.\")))}};function r(e,n){throw new Error(\"Expected \"+(n?\"component\":\"object\")+\" `\"+e+\"` to be defined: you likely forgot to import, pass, or provide it.\")}})(),s})()));","frontmatter":{"id":"esG7-06-06-token-filter","slug":"/krEsguide7/esG7-06-06-token-filter","title":"6.6 토큰 필터 - Token Filter","description":"모든 문서에 대한 저작권은 Elastic 과 김종민(kimjmin@gmail.com) 에게 있으며 허가되지 않은 무단 복제나 배포 및 출판을 금지합니다. 본 문서의 내용 및 포함된 자료를 인용하고자 하는 경우 출처를 명시하고 게재된 주소를 김종민(kimjmin@gmail.com)에게 알려주시기 바랍니다.","date":"2022-01-25T00:00:00.000Z","tags":["indexing","text","analysis","token","filter"],"link":"https://github.com/elastic/docs-kr/esguide7/blob/main","linkPath":"06-text-analysis/06-06-token-filter.mdx"},"missionId":"krEsguide7"},"__N_SSG":true}